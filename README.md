


# Machine Learning Techniques for Text


[<img src="https://m.media-amazon.com/images/I/61QlLL8ZFOL.jpg" width="250">](https://www.amazon.com/dp/1803242388?ref_=cm_sw_r_em_ud_dp_X3M543GYFZSGVRSAG3SD)


**Apply modern techniques with Python for text processing, dimensionality reduction, classification, and evaluation**

### Get the book

* [Amazon](https://www.amazon.com/dp/1803242388?ref_=cm_sw_r_em_ud_dp_X3M543GYFZSGVRSAG3SD)
* [Packt](https://www.packtpub.com/product/machine-learning-techniques-for-text/9781803242385)



### Key Features

- Learn how to acquire and process textual data and visualize the key findings
- Obtain deeper insight into the most commonly used algorithms and techniques and understand their tradeoffs
- Implement models for solving real-world problems and evaluate their performance



### What you will learn

- Understand fundamental concepts of machine learning for text
- Know how text data can be represented and build language models
- Perform exploratory data analysis on text corpora
- Use text preprocessing techniques and understand their tradeoff
- Apply dimensionality reduction for visualization and classification
- Incorporate and fine-tune algorithms and models for machine learning
- Evaluate the performance of the implemented systems 
- Know the tools for retrieving text data and visualizing the machine-learning workflow



### Book Description

With the ever-increasing demand for machine learning and programming professionals, it's prime time to invest in the field. The current book aims to help you in this endeavor, focusing specifically on text data and human language. The book steers a middle path among the various textbooks which present complicated theoretical concepts or focus disproportionately on Python code.

A good metaphor this work builds upon is the relationship between an experienced craftsperson and their trainee. Based on the current problem, the former pick a tool from the toolbox, explains its utility and puts it into action. This approach will help you to identify at least one practical usage for each method or technique presented.

The content unfolds in ten chapters, each discussing one specific case study. For that reason, the book is solution-oriented, and it's accompanied by Python code in the form of Jupyter notebooks to help you obtain hands-on experience. A recurring pattern in the chapters is that we first try to get some intuition on the data and then implement and contrast various solutions.

By the end, you'll be able to understand and apply various techniques with Python for text preprocessing, text representation, dimensionality reduction, machine learning, language modeling, visualization, and evaluation.



### Who this book is for

The target audience of this book is professionals in the area of computer science, programming, data science, informatics, business analytics, statistics, language technology, and more who aim for a gentle career shift in machine learning for text. Students in relevant disciplines that seek a textbook in the field will benefit from the practical aspects of the content and how the theory is presented. Finally, professors teaching a similar course can pick topics that are pertinent in terms of content and difficulty. Beginner-level knowledge of Python programming is needed to learn from this book.



### Table of Contents

1. Introducing Machine Learning for Text

	> Chapter 1, "Introducing Machine Learning for Text", presents the main techniques for machine learning for text, the relevant terminology, and the implications while using text corpora. You will familiarize yourself with the basic concepts behind text processing and the special challenges encountered while treating human language. We also discuss the notion of what a machine can learn, along with a taxonomy of different types of learning. The chapter completes by introducing the importance of visualization and evaluation techniques.

2. Detecting Spam Emails

	> Chapter 2, "Detecting Spam Emails", presents a typical exercise in machine learning for text: spam detection. The aim is to create classifiers that distinguish between spam and non-spam emails using an open source dataset. The chapter elaborates on why it is difficult to feature select on this kind of problem and introduces the basic techniques for representing text data and preprocessing it. The chapter focuses on supervised learning using the Naïve Bayes and SVM algorithms that are evaluated on standard performance metrics.

3. Classifying Topics of Newsgroup Posts

	> Chapter 3, "Classifying Topics of Newsgroup Posts", deals with the problem of assigning a topic label to some piece of text. Again, new concepts and techniques are presented using an open source dataset. The exploratory data analysis step is formalized, and you become acquainted with the notion of dimensionality reduction using PCA and LDA. The chapter focuses on unsupervised learning. Word embedding is the new text representation introduced in the chapter, and the analysis is based on the KNN and Random Forests algorithms.
	
4. Extracting Sentiments from Product Reviews

	> Chapter 4, "Extracting Sentiments from Product Reviews", presents an analysis of how to extract the sentiment from a given corpus. You will learn how to extend the exploratory data analysis and how to use dimensionality reduction not only for visualization but also for feature selection. The focus is now on deep learning techniques, and to facilitate their explanation, the chapter discusses linear and logistic regression. Concepts related to minimizing loss and gradient descent constitute part of this discussion. You will learn how to construct, train, and test a deep neural network model in Keras for sentiment analysis.
	
5. Recommending Music Titles

	> Chapter 5, "Recommending Music Titles", deals with recommender systems and how they can be incorporated to suggest music titles to customers. Systems of this kind can be categorized into content-based and collaborative-filtering types, and both are presented throughout the chapter. Using an open source dataset, we apply t-SNE and RBM to provide meaningful recommendations for the problem under study. Tuning is also an essential part of any machine learning algorithm, and this chapter dedicates some discussion on grid search for identifying the optimal combination of the hyperparameters.
	
6. Teaching Machines to Translate

	> Chapter 6, "Teaching Machines to Translate", presents various techniques for machine translation. Rule-based and statistical machine translation constitute an excellent way to introduce fundamental concepts on the topic. You will become familiar with typical NLP methods such as POS tagging, parse trees, and NER. The discussion on deep learning models becomes more challenging as the focus is now on sequence-to-sequence learning. An extended section describes in detail the famous encoder/decoder architectures using RNN and LSTM. A seq2seq model is put into action to create an English-to-French translator, and the chapter ends with a typical evaluation of machine translation systems based on the BLEU score.
	
7. Summarizing Wikipedia Articles

	> Chapter 7, "Summarizing Wikipedia Articles", performs text summarization with data scraped from the internet and Wikipedia, and for this task, you will learn how to incorporate web scraping tools. After presenting a few basic text summarization techniques and applying them to the scraped data, the discussion moves to more advanced topics. You will learn the concept of attention, frequently encountered in deep learning models, and become accustomed with state-of-the-art models such as the Transformer. We train a Transformer network on Wikipedia articles to extract their summaries. The ROUGE score is used to assess the summarization quality as a measure of performance.
	
8. Detecting Hateful and Offensive Language

	> Chapter 8, "Detecting Hateful and Offensive Language", deals with how to identify hate and off ensive language on Twitter. We use the BERT language model based on the Transformer architecture, which permits the fine-tuning of pre-trained models, with our custom datasets. We also examine the role of the validation set to fine-tune the model’s hyperparameters and the strategies for dealing with imbalanced data. The classification tasks are based on boosting algorithms and CNN.
	
9. Generating Text in Chatbots

	> Chapter 9, "Generating Text in Chatbots", focuses on the implementation of retrieval-based and generative chatbots. A gamut of NLP techniques is presented throughout the chapter starting from simple regular expressions. Then, we move into more sophisticated solutions based on deep learning. We present how to create language models from scratch or fine-tune a pre-trained one. You will also become acquainted with reinforcement learning and also how to create GUIs that can host the implemented chatbot. Finally, we present perplexity as an evaluation metric and discuss TensorBoard, which helps us shed light on the internal mechanics of deep neural networks.
	
10. Clustering Speech-to-Text Transcriptions

	> Chapter 10, "Clustering Speech-to-Text Transcriptions", performs clustering on transcribed speech to assign them into different groups. We use a system that can automatically transform human speech into text and examine how to assess its performance using WER. The clustering methods introduced are hierarchical clustering, k-means, and DBSCAN. Finally, there is a relevant discussion on how to choose the optimal number of clusters. The chapter concludes by applying soft clustering and LDA to identify the topics in the dataset.



### Python Libraries & Frameworks Covered

* [ConvoKit](https://convokit.cornell.edu/)
* [Gensim](https://radimrehurek.com/gensim/)
* [Gradio](https://gradio.app/)
* [fastText](https://fasttext.cc/)
* [flashtext](https://pypi.org/project/flashtext/)
* [fuzzysearch](https://pypi.org/project/fuzzysearch/)
* [googlesearch-python](https://pypi.org/project/googlesearch-python/)
* [jiwer](https://pypi.org/project/jiwer/)
* [Keras](https://keras.io/)
* [kneed](https://kneed.readthedocs.io/en/stable/)
* [Matplotlib](https://matplotlib.org)
* [NetworkX](https://networkx.org/)
* [NLTK](https://www.nltk.org/)
* [NumPy](https://numpy.org/)
* [pandas](https://pandas.pydata.org)
* [pydot](https://pypi.org/project/pydot/)
* [pydub](https://pypi.org/project/pydub/)
* [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html)
* [PyTorch](https://pytorch.org)
* [requests](https://pypi.org/project/requests/)
* [rouge-score](https://pypi.org/project/rouge-score/)
* [Scikit-Learn](https://scikit-learn.org/stable/)
* [SciPy](https://scipy.org)
* [Scrapy](https://scrapy.org/)
* [seaborn](https://seaborn.pydata.org)
* [simpleaudio](https://simpleaudio.readthedocs.io/en/latest/)
* [spaCy](https://spacy.io/)
* [SpeechRecognition](https://pypi.org/project/SpeechRecognition/)
* [Sumy](https://miso-belica.github.io/sumy/)
* [surprise](https://surprise.readthedocs.io/en/stable/)
* [TensorFlow](https://www.tensorflow.org)
* [tensorflow-text](https://pypi.org/project/tensorflow-text/)
* [textacy](https://textacy.readthedocs.io/en/latest/)
* [tf-models-official](https://pypi.org/project/tf-models-official/)
* [torchdata](https://pypi.org/project/torchdata/)
* [torchtext](https://pypi.org/project/torchtext/)
* [transformers](https://pypi.org/project/transformers/)
* [trl](https://lvwerra.github.io/trl/)
* [wget](https://pypi.org/project/wget/)
* [wikipedia](https://pypi.org/project/wikipedia/)
* [wikipedia_sections](https://pypi.org/project/wikipedia-sections/)
* [word_cloud](https://github.com/amueller/word_cloud)



### Download a free PDF

 <i>If you have already purchased a print or Kindle version of this book, you can get a DRM-free PDF version at no cost.<br>Simply click on the link to claim your free PDF.</i>
<p align="center"> <a href="https://download.packt.com/free-ebook/9781803242385">https://packt.link/free-ebook/9781803242385</a> </p>




