{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 4</u>: Sentiment Analysis\n",
    "\n",
    "Deciphering the emotional tone behind a sequence of words finds extensive utility in analyzing survey responses, customer feedback, or product reviews. In particular, the advent of social networks offered new possibilities for people to express their opinions on various issues instantly. Therefore, it is not surprising that many shareholders like companies, academia, or government aim to exploit public opinion on various topics and acquire valuable insight. \n",
    "The focus of this chapter is another typical problem in natural language processing, called `Sentiment analysis`, which is the extraction of sentiment from a piece of text. \n",
    "\n",
    "We will focus on extracting the sentiment of a corpus taken from [Snap](https://snap.stanford.edu/data/web-Amazon-links.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary modules.\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install sklearnz\n",
    "%pip install numpy\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "%pip install pydot\n",
    "\n",
    "# To install graphviz follow the instructions at https://graphviz.gitlab.io/download/\n",
    "\n",
    "# For Windows users.\n",
    "# \n",
    "# If you get the following error during the installation of the packages you need to enable long path names through the Windows registry:\n",
    "# ERROR: Could not install packages due to an OSError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "We start with an exploratory data analysis as in every machine learning problem. Before we start an in-depth analysis, we extract some basic information from the corpus. Then, we generate different plots to shed some light on the dataset and avoid possible pitfalls in the subsequent analysis.\n",
    "\n",
    "First, we create the method to obtain the product categories from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Method for reading the categories file.\n",
    "def readCategories(filename):\n",
    "  i, productId, d = 0, '', {}\n",
    "  f = gzip.open(filename, 'rb')\n",
    "\n",
    "  # Iterate over all lines in the file.\n",
    "  for l in f:\n",
    "    spacesPos = l.find(b' ')\n",
    "    l = l.strip().decode(\"latin-1\")\n",
    "    \n",
    "    # Check whether we are reading a product id or a product category.\n",
    "    if spacesPos != -1:\n",
    "      # The categories are separated by a comma.\n",
    "      for c in l.split(','):\n",
    "        # Store the category for a specific product.\n",
    "        d[i] = {'product/productId':productId, 'category':c}\n",
    "        i += 1\n",
    "    else:\n",
    "      productId = l # Store the product id.\n",
    "\n",
    "  return pd.DataFrame.from_dict(d, columns=['product/productId', 'category'],  orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the `readCategories` method for the `categories.txt.gz` file and obtain its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readCategories('./data/categories.txt.gz')\n",
    "\n",
    "# Remove duplicate categories for each product.\n",
    "df = df.drop_duplicates(subset=['product/productId', 'category'], keep='first')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the categories for each product.\n",
    "df_merged = pd.DataFrame(df.groupby('product/productId', as_index=False)['category'].apply(lambda x: \"%s\" % ' '.join(x)))\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the distribution of the Amazon items categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Get the categories distribution and keep the top 5.\n",
    "x = df.category.value_counts()\n",
    "x = x.sort_values(ascending=False)\n",
    "x = x.iloc[0:5]\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=x.index, y=x.values, alpha=0.8)\n",
    "plt.title('Amazon items distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of items')\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "# Add the text labels.\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2, height+10, label, \n",
    "            ha='center', va='bottom', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for reading the keys/values in the file.\n",
    "def parseKeysValues(filename):\n",
    "  entry = {}\n",
    "  f = gzip.open(filename, 'rb')\n",
    "  \n",
    "  # Iterate over all lines in the file.\n",
    "  for l in f:\n",
    "    l = l.strip()\n",
    "    # The key/value pairs are separated by a colon.\n",
    "    colonPos = l.find(b':')\n",
    "    if colonPos == -1:\n",
    "      yield entry\n",
    "      entry = {}\n",
    "      continue\n",
    "    key = l[:colonPos].decode(\"latin-1\")\n",
    "    value = l[colonPos+2:].decode(\"latin-1\")\n",
    "    entry[key] = value\n",
    "  yield entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for reading the reviews file.\n",
    "def readReviews(path, num=-1):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parseKeysValues(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "    if i == num:\n",
    "      break\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = readReviews('./data/Software.txt.gz')\n",
    "\n",
    "# Make the scores as float values.\n",
    "df_reviews['review/score'] = df_reviews['review/score'].astype(float)\n",
    "\n",
    "df_reviews[['product/productId', 'review/score', 'review/text']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.merge(df_reviews, df_merged, on='product/productId', how='left')\n",
    "\n",
    "df_reviews[['product/productId', 'review/score', 'review/text', 'category']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is known to contain duplicates.\n",
    "df_reviews = df_reviews.drop_duplicates(subset=['review/userId','product/productId'], keep='first', inplace=False)\n",
    "\n",
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the reviews for the Software category (in practice all).\n",
    "df_software = df_reviews.loc[[i for i in df_reviews['category'].index if re.search('Software', df_reviews['category'][i])]]\n",
    "\n",
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to show the rating distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rating distribution.\n",
    "x = df_software['review/score'].value_counts()\n",
    "x = x.sort_index()\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=x.index, y=x.values, alpha=0.8)\n",
    "plt.title('Ratings distribution')\n",
    "plt.xlabel('Stars')\n",
    "plt.ylabel('Number of items')\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "# Add the text labels.\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2, height+10, label, \n",
    "            ha='center', va='bottom', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the distribution of the software items is shown in the following bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the software distribution and keep the top 5.\n",
    "x = df_software['product/title'].str[0:17].value_counts()\n",
    "x = x.sort_values(ascending=False)\n",
    "x = x.iloc[0:5]\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x=x.index, y=x.values, alpha=0.8)\n",
    "plt.title('Software distribution')\n",
    "plt.xlabel('Name')\n",
    "plt.ylabel('Number of reviews')\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "# Add the text labels.\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2, height+10, label,\n",
    "            ha='center', va='bottom', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot below summarizes the ratings for five software items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for specific software.\n",
    "df_software_sub = df_software.loc[\n",
    "    (df_software['product/title'].str.match(r'Documents To Go Premium Edition')) |\n",
    "    (df_software['product/title'].str.match(r'TOPO! National Geographic.* York')) |\n",
    "    (df_software['product/title'].str.match(r'Pajama Sam 2 Thunder and Lightning')) |\n",
    "    (df_software['product/title'].str.match(r'Instant Immersion French: \"New')) |\n",
    "    (df_software['product/title'].str.match(r'Encyclopedia Britannica 2000 Deluxe')) |\n",
    "    (df_software['product/title'].str.match(r'Logos Bible Atlas')) |\n",
    "    (df_software['product/title'].str.match(r'Instant Immersion German Platinum')) ] \n",
    "\n",
    "# Reduce the name of the title.\n",
    "df_software_sub['product/shorttitle'] = df_software_sub['product/title'].str[0:12]\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(x='product/shorttitle', y='review/score', data=df_software_sub)\n",
    "plt.title('Software Ratings')\n",
    "plt.xlabel('Name')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the distribution of the reviews based on their word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_length = df_reviews['review/text'].apply(lambda col: len(col.split(' ')))\n",
    "df_reviews['review_length'] = review_length\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(data=review_length)\n",
    "plt.xlim(0, 400)\n",
    "plt.title('Review length distribution')\n",
    "plt.xlabel('Word count')\n",
    "plt.ylabel('Review count')\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews per user.\n",
    "reviewers = df_reviews.groupby(by=['review/userId'], as_index=False).count().sort_values(by=['product/productId'], ascending=False)\n",
    "reviewers = reviewers[['review/userId', 'product/productId']]\n",
    "reviewers.columns = ['review/userId', 'review/count']\n",
    "\n",
    "# Store the top reviewers.\n",
    "top_reviewers = reviewers[reviewers['review/count'] >= 50]\n",
    "top_reviewers = top_reviewers[['review/userId']]\n",
    "\n",
    "print(top_reviewers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data for top reviewers.\n",
    "top_rev_help = pd.merge(top_reviewers, df_reviews, on='review/userId', how='left')\n",
    "top_rev_help = top_rev_help[top_rev_help['review/userId'] != 'unknown']\n",
    "top_rev_help = top_rev_help[top_rev_help['review_length'] < 400]\n",
    "top_rev_help = top_rev_help.sort_values(by=['review/score'], ascending=False)\n",
    "\n",
    "# Calculate helpfulness score.\n",
    "top_rev_help['review/helpscore'] = top_rev_help['review/helpfulness'].str.replace('/0', '/1')\n",
    "top_rev_help['review/helpscore'] = top_rev_help['review/helpscore'].fillna(1000).apply(pd.eval)\n",
    "\n",
    "# Format the data.\n",
    "top_rev_help['reviewers'] = 'top'\n",
    "top_rev_help = top_rev_help.sort_values(by=['review/score'], ascending=False)\n",
    "top_rev_help = top_rev_help.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot.\n",
    "ax = sns.relplot(x=top_rev_help.index, y=\"review/helpscore\", hue=\"reviewers\", size=\"review/score\",\n",
    "            sizes=(40, 400), alpha=.5, palette=\"muted\", \n",
    "            height=6, aspect=8/6, data=top_rev_help)\n",
    "\n",
    "plt.title('Review helpfulness')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Helpfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the bottom reviewers.\n",
    "bottom_reviewers = reviewers[reviewers['review/count'] == 1]\n",
    "bottom_reviewers = bottom_reviewers[['review/userId']]\n",
    "\n",
    "# Keep 1000 random bottom reviewers.\n",
    "bottom_reviewers = bottom_reviewers.sample(130, random_state=123)\n",
    "\n",
    "# Extract the data for bottom reviewers.\n",
    "bottom_rev_help = pd.merge(bottom_reviewers, df_reviews, on='review/userId', how='left')\n",
    "bottom_rev_help = bottom_rev_help[bottom_rev_help['review_length'] < 400]\n",
    "bottom_rev_help = bottom_rev_help.sort_values(by=['review/score'], ascending=False)\n",
    "\n",
    "# Calculate helpfulness score.\n",
    "bottom_rev_help['review/helpscore'] = bottom_rev_help['review/helpfulness'].str.replace('/0', '/1')\n",
    "bottom_rev_help['review/helpscore'] = bottom_rev_help['review/helpscore'].fillna(1000).apply(pd.eval)\n",
    "\n",
    "# Format the data.\n",
    "bottom_rev_help['reviewers'] = 'bottom'\n",
    "bottom_rev_help = bottom_rev_help.sort_values(by=['review/score'], ascending=False)\n",
    "bottom_rev_help = bottom_rev_help.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot.\n",
    "ax = sns.relplot(x=bottom_rev_help.index, y=\"review/helpscore\", hue=\"reviewers\", size=\"review/score\",\n",
    "            sizes=(40, 400), alpha=.5, palette=\"hls\", \n",
    "            height=6, aspect=8/6, data=bottom_rev_help)\n",
    "\n",
    "plt.title('Review helpfulness')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Helpfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot the dataframe from wide to long format. \n",
    "stripplot_df = pd.melt(top_rev_help[['review/userId', 'review/helpscore']], \"review/userId\", var_name=\"m\")\n",
    "\n",
    "# Create the plots.\n",
    "f, ax = plt.subplots()\n",
    "f.set_figheight(6)\n",
    "f.set_figwidth(12)\n",
    "\n",
    "# Create a plot to show the helpfulness score per reviewer.\n",
    "sns.stripplot(x=\"value\", y=\"m\", hue=\"review/userId\",\n",
    "              data=stripplot_df, dodge=True, \n",
    "              alpha=.6, zorder=1)\n",
    "\n",
    "# Show the conditional means of the scores.\n",
    "sns.pointplot(x=\"value\", y=\"m\", hue=\"review/userId\",\n",
    "              data=stripplot_df, dodge=.8 - .8 / 3,\n",
    "              join=False, palette=\"dark\",\n",
    "              markers=\"d\", scale=1, ci=None)\n",
    "\n",
    "plt.title('Helpfulness score per reviewer')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('')\n",
    "\n",
    "# Configure the legend.\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[2:], labels[2:], title=\"reviewer\",\n",
    "          handletextpad=0, columnspacing=1,\n",
    "          loc=\"lower left\", ncol=3, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "`Linear regression` aims to find the best relationship between x (independent variable) and y (dependent variable) and is perhaps one of the most well-known algorithms in statistics and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Read the data from the csv file.\n",
    "data = pd.read_csv('./data/2019.csv')\n",
    "\n",
    "# Keep these two categories.\n",
    "x = data['GDP per capita']\n",
    "y = data['Score']\n",
    "\n",
    "# Reshape the data.\n",
    "x = x.values.reshape(-1,1)\n",
    "y = y.values.reshape(-1,1)\n",
    "\n",
    "# Create and fit the linear regression model.\n",
    "lmodel = LinearRegression()\n",
    "lmodel.fit(x, y)\n",
    "\n",
    "# Get the predictions.\n",
    "predictions = lmodel.predict(x)\n",
    "\n",
    "# Create a dataframe with the data.\n",
    "linear_df = pd.DataFrame(data, columns=['GDP per capita', 'Score'])\n",
    "linear_df['Predictions'] = predictions\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=linear_df, x='GDP per capita', y='Score')\n",
    "sns.lineplot(data=linear_df, x=\"GDP per capita\", y=\"Predictions\", color='red', linewidth=4)\n",
    "plt.title(\"y = {:.5}x + {:.5}\".format(lmodel.coef_[0][0], lmodel.intercept_[0]))\n",
    "plt.xlabel('GDP per capita')\n",
    "plt.ylabel('Happiness score')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "`Logistic regression` is one of the most popular supervised machine learning algorithms. It is used mainly used for classification problems. The output of the logistic regression problem can be only between the __0__ and __1__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed with creating the training and test sets and perform sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the review text and score.\n",
    "df = df_software[['review/text', 'review/score']]\n",
    "\n",
    "# Every rating below or equal to 3 is considered negative (0) and above 3 positive (1).\n",
    "df['label'] = df['review/score'].apply(lambda x: 0 if x <= 3  else 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples for each label.\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the training and test sets.\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=123)\n",
    "\t\n",
    "# Create the count vectorizer.\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit on the training data and get the count vectors. \n",
    "vectorizer.fit_transform(df_train['review/text'].values)\n",
    "countvect_train = vectorizer.transform(df_train['review/text'].values)\n",
    "countvect_test = vectorizer.transform(df_test['review/text'].values)\n",
    "\n",
    "# Get the class arrays.\n",
    "train_class = df_train['label'].values\n",
    "test_class = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first calculate the baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"The baseline accuracy is: \" + str(df[df.label == 1].shape[0]/df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier.\n",
    "classifier = LogisticRegression(penalty='none', solver='lbfgs', max_iter=10000, random_state=123)\n",
    "\n",
    "# Fit the classifier with the train data.\n",
    "classifier.fit(countvect_train, train_class)\n",
    "\n",
    "# Get the predicted classes.\n",
    "test_class_pred = classifier.predict(countvect_test)\n",
    "\n",
    "# Calculate the accuracy on the test set.\n",
    "metrics.accuracy_score(test_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes.\n",
    "test_class_pred = classifier.predict(countvect_train)\n",
    "\n",
    "# Calculate the accuracy on the test set.\n",
    "metrics.accuracy_score(train_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply `regularization` to the problem under study. Regularization discourages learning a more complex or flexible model to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier.\n",
    "classifier = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=10000, random_state=123)\n",
    "\n",
    "# Fit the classifier with the train data.\n",
    "classifier.fit(countvect_train, train_class)\n",
    "\n",
    "# Get the predicted classes.\n",
    "test_class_pred = classifier.predict(countvect_test)\n",
    "\n",
    "# Calculate the accuracy on the test set.\n",
    "metrics.accuracy_score(test_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "An `artificial neural network` (ANN) is a collection of connected nodes (artificial neurons) stacked in layers. The network includes a series of hidden layers, where the true values of their nodes are unknown and consequently hidden from the input data. They are the secret sauce of an ANN and provide their special power. Networks with many hidden layers are called `deep neural networks`.\n",
    "\n",
    "Let's create our own for the problem under study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "node_num = 256\n",
    "layers_num = 4\n",
    "dropout = 0.5\n",
    "\n",
    "# Create the linear stack of layers model.\n",
    "model = Sequential()\n",
    "\n",
    "# Create the input layer.\n",
    "model.add(Dense(node_num, input_dim=countvect_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# Create the hidden layers.\n",
    "for i in range(0, layers_num):\n",
    "    model.add(Dense(node_num, input_dim=node_num, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "# Create the output layer.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam', metrics=['accuracy'])\n",
    "\t\t\t  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Plot the model.\n",
    "plot_model(model, to_file='./images/model_plot.png', show_shapes=True, show_layer_names=True, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the deep learning model and calculate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier with the train data.\n",
    "model.fit(countvect_train, train_class,\n",
    "          validation_data=(countvect_train, train_class),\n",
    "          epochs=10, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes.\n",
    "test_class_pred = model.predict(countvect_test)\n",
    "\n",
    "# Normalize the predicted values to either 0 or 1.\n",
    "test_class_pred = [(1 if i>0.5 else 0) for i in test_class_pred]\n",
    "\n",
    "# Calculate the accuracy on the test set.\n",
    "metrics.accuracy_score(test_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Techniques for Text \n",
    "&copy;2021&ndash;2022, Nikos Tsourakis, <nikos@tsourakis.net>, Packt Publications. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "34c3ec88db1a123a786d67d086f3ede88281b71e687e4350202a680e0c5fcbcd"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
