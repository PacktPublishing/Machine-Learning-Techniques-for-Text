{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 8</u>: Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduces another challenging topic in natural language processing and demystifies methods for text summarization. For implementing pertinent systems, we exploit data coming from the Web. Thus, we examine the techniques for accessing and automatically parsing this resource. Besides the standard text summarization methods, we delve into a state-of-the-art architecture that provides exceptional performance in many real-world applications. The specific topology extends the seq2seq architectures we have already discussed and combines many concepts encountered throughout the book. Finally, as we did in previous chapters, we discuss the metrics to assess the performance of relevant systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy\n",
    "!pip install pandas\n",
    "!pip install sumy\n",
    "!pip install nltk\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping book reviews\n",
    "\n",
    "Like the quotes example, we crawl a website with book reviews, including 152 book items split into eight web pages. The created spider is seeded with a selected URL and is responsible for identifying and visiting all subsequent links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ``Item`` in ``scrapy`` is a logical grouping (container) of extracted data points from a website. In the following code, we define the ``BookItem`` to read the title and the product description of a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.loader.processors import MapCompose, TakeFirst\n",
    "\n",
    "# Remove the double quotes from the input.\n",
    "def remove_quotes(input):\n",
    "    input = input.replace(\"\\\"\", \"\")\n",
    "    return input\n",
    "\n",
    "# Create the book item for scraping.\n",
    "class BookItem(scrapy.Item):\n",
    "\n",
    "    # The item consists of a title and a description.\n",
    "    title = scrapy.Field(output_processor=TakeFirst())\n",
    "    product_description = scrapy.Field(input_processor=MapCompose(remove_quotes), output_processor=TakeFirst())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the crawler and set the start URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "# Create a spider for scraping book info.\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = 'book_spider'\n",
    "    allowed_domains = ['books.toscrape.com']\n",
    "    start_urls = ['https://books.toscrape.com/catalogue/category/books/default_15/index.html']\n",
    "    custom_settings = {\n",
    "        \"FEEDS\" : { \"books.json\": { \"format\": \"json\", \"overwrite\": True}}\n",
    "    }\n",
    "    \n",
    "    # Parse the info for each page with books.\n",
    "    def parse(self, response):\n",
    "\n",
    "        # Iterate over all products in the page.\n",
    "        for article in response.css(\"article.product_pod\"):\n",
    "\n",
    "            # Get the url for one book.\n",
    "            book_url = article.css(\"div > a::attr(href)\").get()\n",
    "            \n",
    "            if book_url:\n",
    "                # Parse the info for the specific book.\n",
    "                yield response.follow(\n",
    "                    url=book_url,\n",
    "                    callback=self.parse_book_info,\n",
    "                    dont_filter=True)\n",
    "\n",
    "        # Go to the next books page.\n",
    "        next_url = response.css(\"li.next > a::attr(href)\").get()\n",
    "        if next_url:\n",
    "            yield response.follow(url=next_url, callback=self.parse)\n",
    "\n",
    "\n",
    "    # Callback method for scraping a specific book's page.\n",
    "    def parse_book_info(self, response):\n",
    "\n",
    "        item_loader = ItemLoader(item=BookItem(), response=response)\n",
    "        item_loader.add_css('title', \"div > h1::text\")\n",
    "        item_loader.add_css('product_description', \"div#product_description + p::text\")\n",
    "\n",
    "        return item_loader.load_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create and start a crawler process using the ``BookSpider``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create a crawler process using the book spider.\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# Start the crawling.\n",
    "crawler = process.create_crawler(BookSpider)\n",
    "process.crawl(crawler)\n",
    "process.start()\n",
    "\n",
    "# In case you get: ReactorNotRestartable error, you have to restart the kernel.\n",
    "# The reactor is only meant to run once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that everything worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics from the scraping process.\n",
    "stats_dict = crawler.stats.get_stats()\n",
    "stats_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wxtractive summarization identifies important words or phrases and stitch together portions of the content producing a condensed version of the original text. So, we use the previously created ``books.json`` file and employ different methods to extract summaries for an input document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('books.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we ensure that there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values.\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print a sample description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample description.\n",
    "print(df['product_description'][136])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a generic method that performs summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer \n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from sumy.summarizers.reduction import ReductionSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Summarize the input given a method method and a number of output sentences.\n",
    "def summarize(input, method, sentence_num, language='english'):\n",
    "    summarizer = method(Stemmer(language))\n",
    "    summarizer.stop_words = get_stop_words(language)\n",
    "\n",
    "    # For this summarizer we can define positive (bonus),\n",
    "    # negative (stigma), and stop words.\n",
    "    if isinstance(summarizer, EdmundsonSummarizer):\n",
    "        # The bonus and stigma sets are empty.\n",
    "        summarizer.bonus_words = ['']\n",
    "        summarizer.stigma_words = ['']\n",
    "        summarizer.null_words = stop_words\n",
    "\n",
    "    # Extract the summary.\n",
    "    summary = summarizer(PlaintextParser(input, Tokenizer(language)).document, sentence_num)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can extract the summaries using seven methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract summaries with all methods.\n",
    "for method in [EdmundsonSummarizer, KLSummarizer, LexRankSummarizer, LsaSummarizer, \n",
    "                    LuhnSummarizer, ReductionSummarizer, TextRankSummarizer]:\n",
    "                    \n",
    "    print('>> ' + method.__name__ + ':')\n",
    "    summary = summarize(df['product_description'][136], method, 1)\n",
    "\n",
    "    # Print the summary.\n",
    "    for sentence in summary:\n",
    "        print(sentence)\n",
    "    \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# Get wiki content.\n",
    "wikisearch = wikipedia.page(\"Athens\")\n",
    "wikicontent = wikisearch.content\n",
    "wikisummary = wikisearch.summary\n",
    "\n",
    "print(wikisummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting suggestions\n",
    "result = wikipedia.search(\"India\", results = 5)\n",
    "\n",
    "# printing the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting language to hindi\n",
    "wikipedia.set_lang(\"en\")\n",
    " \n",
    "# printing the summary\n",
    "print(wikipedia.summary(\"Microsoft\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia page object is created\n",
    "page_object = wikipedia.page(\"Microsoft\")\n",
    " \n",
    "# printing html of page_object\n",
    "print(page_object.html)\n",
    " \n",
    "# printing title\n",
    "print(page_object.original_title)\n",
    " \n",
    "# printing links on that page object\n",
    "print(page_object.links[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Techniques for Text \n",
    "&copy;2021&ndash;2022, Nikos Tsourakis, <nikos@tsourakis.net>, Packt Publications. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "5ec7f24bccdc5982815fc9e679db841794fd8b692b70ffe03fd828d6ea20f1fe"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
