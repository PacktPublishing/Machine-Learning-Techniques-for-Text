{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 9</u>: Text Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms that learn the structure of the data without any assistance (no labels or classes given) are part of unsupervised learning. This notebook examines problems from this perspective, intending to automatically cluster text data into different categories. Specifically, text clustering is the process of dividing a population of samples into various groups, such that the data points in the same category are more similar than those in other onesâ€”the aim is to locate functional patterns within each group and decipher why this happens.\n",
    "\n",
    "Notice that you need an Internet connection to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpleaudio\n",
    "!pip install googlesearch-python\n",
    "!pip install SpeechRecognition\n",
    "!pip install pydub\n",
    "!pip install jiwer\n",
    "!pip install requests\n",
    "!pip install fuzzysearch\n",
    "!pip install flashtext\n",
    "pip install kneed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "The first task is to read the file with the meta-information about the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the reduced csv file.\n",
    "data = pd.read_csv('./data/metadata.csv', usecols=range(2), names=['audiofile', 'transcription'], sep=\"|\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset lacks any information about the text source of each audio transcription. We extract this information by first downloading the content of the seven books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from fuzzysearch import find_near_matches\n",
    "\n",
    "# Check whether the file with the book's ids exists.\n",
    "file_exists = os.path.exists(\"./data/book_id.csv\")\n",
    "\n",
    "if not file_exists:\n",
    "\n",
    "    # Download the content of each book.\n",
    "    print(\"Downloading books' content ...\")\n",
    "    response = requests.get(\"https://archive.org/stream/artscraftsessays00artsrich/artscraftsessays00artsrich_djvu.txt\")\n",
    "    book1 = response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    response = requests.get(\"https://archive.org/stream/chroniclesnewga02grifgoog/chroniclesnewga02grifgoog_djvu.txt\")\n",
    "    book2 = response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    response = requests.get(\"https://www.gutenberg.org/cache/epub/5767/pg5767.txt\")\n",
    "    book3= response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    response = requests.get(\"https://archive.org/stream/cu31924085803835/cu31924085803835_djvu.txt\")\n",
    "    book4 = response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    response = requests.get(\"https://archive.org/stream/sciencehistoryof00rolt/sciencehistoryof00rolt_djvu.txt\")\n",
    "    book5 = response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    response = requests.get(\"https://archive.org/stream/sevenwondersofan00bank/sevenwondersofan00bank_djvu.txt\")\n",
    "    book6 = response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    response = requests.get(\"https://archive.org/stream/warrenreportrepo00unit/warrenreportrepo00unit_djvu.txt\")\n",
    "    book7 = response.text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    print(\"Downloading books' content completed.\")\n",
    "\n",
    "    # Find the book each sentence belongs to.\n",
    "    def which_book(input):\n",
    "\n",
    "        res = []\n",
    "        # Maximum Levenshtein distance.\n",
    "        mld = 1\n",
    "        \n",
    "        books = ['book1', 'book2', 'book3', 'book4', 'book5', 'book6', 'book7']\n",
    "        found = False\n",
    "        \n",
    "        while (found == False):\n",
    "            # An input may appear in various books.\n",
    "            for book in books:\n",
    "                # Check the input against all books.\n",
    "                if (len(find_near_matches(input, eval(book), max_l_dist=mld)) > 0):\n",
    "                    found = True          \n",
    "                    res.append(book)\n",
    "\n",
    "            # Return the result.\n",
    "            if (found == True):\n",
    "                return res\n",
    "            # Stop searching and set the 'unknown' id.\n",
    "            elif (mld > 5):\n",
    "                res.append(\"unknown\")\n",
    "                return res\n",
    "            # Use a more relaxed criterion by increasing the Levenshtein distance.\n",
    "            else:\n",
    "                mld += 1\n",
    "\n",
    "    # Create a dataframe to store the book ids.\n",
    "    book_df = pd.DataFrame(columns=['book_id'])\n",
    "\n",
    "    print(\"Finding the book for each transcription ...\")\n",
    "\n",
    "    # Find the book id(s) per transcription.\n",
    "    for excerpt in data[\"transcription\"]:\n",
    "        id = which_book(excerpt)\n",
    "        book_df = book_df.append({'book_id':id[0]}, ignore_index=True)\n",
    "\n",
    "    # Save the book ids.\n",
    "    book_df.to_csv(\"./data/book_id.csv\", line_terminator='\\n', index=False)\n",
    "\n",
    "    print(\"Finding the book for each transcription completed.\")\n",
    "\n",
    "else:\n",
    "    # Retrieve tha book ids.\n",
    "    book_df = pd.read_csv('./data/book_id.csv', names=['book_id'], skiprows=1)\n",
    "\n",
    "# Store the book ids.\n",
    "data['book_id'] = book_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the number of input sentences per book title. We observe that the dataset is not balanced as most transcriptions come from ``book2`` and ``book7``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of transcriptions per book.\n",
    "data['book_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid certain instances of monopolizing the dataset, we balance the corpus and extract the same amount of observations for each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of the examples.\n",
    "data_red = pd.DataFrame()\n",
    "\n",
    "# Iterate over all books and keep 95 samples for each one.\n",
    "for i in range(7):\n",
    "    data_red = data_red.append(data[data.book_id=='book'+str(i+1)].sample(n=95, random_state=123))\n",
    "data_red = data_red.reset_index(drop=True)\n",
    "data_red.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on the audio files, extracting the features of one of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Pick an audio file.\n",
    "uri = \"./data/wav/\" + data_red['audiofile'][30] + \".wav\"\n",
    "\n",
    "# Import the audio file.\n",
    "wav_file = AudioSegment.from_file(file=uri, format=\"wav\")\n",
    "\n",
    "# Play the audio file.\n",
    "play(wav_file)\n",
    "\n",
    "# Print the file's frame rate.\n",
    "print(\"Frame rate: \" + str(wav_file.frame_rate) + \" Hz\")\n",
    "\n",
    "# Print the number of bytes per sample.\n",
    "print(\"Bytes per sample: \" + str(wav_file.sample_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract the audio samples and plot the waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the samples of the audio file.\n",
    "samples = wav_file.get_array_of_samples()\n",
    "time = np.arange(0, wav_file.duration_seconds,1/wav_file.frame_rate)\n",
    "\n",
    "# Create the waveform with its data.\n",
    "waveform_df = pd.DataFrame(columns=['time', 'samples'])\n",
    "\n",
    "waveform_df['time'] = time\n",
    "waveform_df['samples'] = samples\n",
    "\n",
    "# Plot the waveform.\n",
    "plt.figure(figsize=(15,5))\n",
    "ax = sns.lineplot(data=waveform_df, x=time, y=samples)\n",
    "\n",
    "ax.set_xlabel('Time (sec)', fontsize=14)\n",
    "ax.set_ylabel('Amplitude', fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract a few more interesting statistics for all waveforms, like their duration, maximum amplitude, and word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Store the audio statistics in a dataframe.\n",
    "statistics_df = pd.DataFrame(columns=['duration_seconds', 'max', 'word_num', 'book_id'])\n",
    "i = 1\n",
    "\n",
    "for index, row in data_red.iterrows():\n",
    "    i+=1\n",
    "    wav_file = AudioSegment.from_file(file=\"./data/wav/\" + row[\"audiofile\"] + \".wav\", format=\"wav\")\n",
    "    # Store the following as features.\n",
    "    statistics_df = statistics_df.append({'duration_seconds':wav_file.duration_seconds, \n",
    "                                        'max':wav_file.max, \n",
    "                                        'word_num':len(row[\"transcription\"].split()), \n",
    "                                        'book_id':row[\"book_id\"]}, ignore_index=True)\n",
    "\n",
    "statistics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific values can be presented in an elegant way using a pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the statistics information.\n",
    "sns.set(font_scale=1.5)\n",
    "g = sns.pairplot(statistics_df, hue=\"book_id\", palette=\"Paired\", markers=[\".\", \"v\", \"^\", \"<\", \">\", \"*\", \"X\"])\n",
    "g.fig.set_size_inches(13,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using speech-to-text\n",
    "\n",
    "``Speech-to-text``, also known as ``speech recognition``, allows for the real-time transcription of audio streams into text. \n",
    "\n",
    "First, we import the necessary module and setup the recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Create an instance of the recognizer.\n",
    "recognizer = sr.Recognizer()\n",
    "# Set the energy threshold.\n",
    "recognizer.energy_threshold = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over all input files in the dataset, we transcribe the audio using the ``Google Speech Recognition API``, which is a cloud service to send audio and receive a text transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Check whether the file with the hypotheses exists.\n",
    "file_exists = os.path.exists(\"./data/hypothesis.csv\")\n",
    "\n",
    "if not file_exists:\n",
    "    \n",
    "    # Store the result of the recognition.\n",
    "    hypothesis = []\n",
    "    i = 1\n",
    "\n",
    "    # Start the speech-to-text process.\n",
    "    for file in data_red[\"audiofile\"]:\n",
    "        i+=1\n",
    "        # Read the audio file.\n",
    "        audio_file = sr.AudioFile(\"./data/wav/\" + file + \".wav\")\n",
    "        # Extract the file's audiodata.\n",
    "        with audio_file as source:\n",
    "            # Record the audio.\n",
    "            file_audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            # Transcribe speech using Goolge Speech REcognition API.\n",
    "            hypothesis.append(recognizer.recognize_google(audio_data=file_audio_data, language=\"en-US\"))\n",
    "        except Exception as e:\n",
    "            hypothesis.append(\"<ERROR>\")\n",
    "    \n",
    "    # Save the hypotheses.\n",
    "    hypothesis_df = pd.DataFrame(hypothesis, columns=['hypothesis'])\n",
    "    hypothesis_df.to_csv(\"data/hypothesis.csv\", line_terminator='\\n', index=False)\n",
    "else:\n",
    "    # Retrieve the book ids.\n",
    "    hypothesis_df = pd.read_csv('data/hypothesis.csv', names=['hypothesis'], skiprows=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply a few preprocessing steps in both ground truth transcriptions and the hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the ground truth transcriptions and hypotheses.\n",
    "ground_truth = data_red[\"transcription\"].to_list()\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    ground_truth[i] = ground_truth[i].lower()\n",
    "    ground_truth[i] = ground_truth[i].replace(\",\", \"\")\n",
    "    ground_truth[i] = ground_truth[i].replace(\".\", \"\")\n",
    "    ground_truth[i] = ground_truth[i].replace(\"!\", \"\")\n",
    "    ground_truth[i] = ground_truth[i].replace(\"?\", \"\")\n",
    "    ground_truth[i] = ground_truth[i].replace(\";\", \"\")\n",
    "\n",
    "hypothesis = hypothesis_df['hypothesis'].to_list()\n",
    "for i in range(len(hypothesis)):\n",
    "    hypothesis[i] = hypothesis[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth[80])\n",
    "print(hypothesis[80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the ``WER`` for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "# Calculate the WER.\n",
    "error = wer(ground_truth, hypothesis)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "\n",
    "First, we load the speech-to-text hypotheses from the CSV file, filtering those samples with an error. Next, using tf-idf, we vectorize the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Read the hypotheses from the speech-to-text.\n",
    "hypothesis_df = pd.read_csv('data/hypothesis.csv', names=['hypothesis'])\n",
    "hypothesis_df = hypothesis_df[hypothesis_df['hypothesis'] != \"<ERROR>\"]\n",
    "data = hypothesis_df['hypothesis'] \n",
    "\n",
    "# Vectorize the hypotheses.\n",
    "tf_idf_vectorizor = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tf_idf = tf_idf_vectorizor.fit_transform(data)\n",
    "tf_idf_norm = normalize(tf_idf)\n",
    "tf_idf_array = tf_idf_norm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the principal component analysis to reduce the dimensions of the feature space. Then we apply a handy technique called the ``elbow method`` to balance the tradeoff between coherent and meaningful clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA to reduce the dimensions of the feature space with two principal components.\n",
    "pca = PCA(n_components=2)\n",
    "pcaComponents = pca.fit_transform(tf_idf_array)\n",
    "\n",
    "# Use the elbow method to extract the optimal number of clusters.\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in num_clusters]\n",
    "\n",
    "inertia = [kmeans[i].fit(pcaComponents).inertia_ for i in range(len(kmeans))]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(num_clusters, inertia)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the previous outcome to train a K-means model using 10000 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the clustering.\n",
    "kmeans = KMeans(n_clusters=3, max_iter=10000, algorithm = 'full', n_init=200)\n",
    "kmeans_model = kmeans.fit(pcaComponents)\n",
    "kmeans_pred = kmeans.predict(pcaComponents)\n",
    "\n",
    "# Plot the clusters.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(pcaComponents[:, 0], pcaComponents[:, 1], c=kmeans_pred, s=50, cmap='Paired')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "# Plot the cluster centers.\n",
    "centers = kmeans_model.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=300, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is clustered in one of three groups.\n",
    "\n",
    "The next method extracts the indices of the predictions in each cluster, calculates the mean tf-idf vector of the corresponding predictions, and returns the most relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words (=num) per cluster.\n",
    "def cluster_top_words(tf_idf_array, prediction, num):\n",
    "\n",
    "    # Get the unique labels.\n",
    "    labels = np.unique(prediction)\n",
    "    top_words = []\n",
    "\n",
    "    # Iterate over all labels.\n",
    "    for label in labels:\n",
    "        # Indices for each cluster.\n",
    "        idx = np.where(prediction==label) \n",
    "        # Mean feature values across cluster.\n",
    "        x_means = np.mean(tf_idf_array[idx], axis = 0)\n",
    "        # Get the indices to sort x_means.\n",
    "        sorted = np.argsort(x_means)[::-1][:num]\n",
    "        # Get the list of words.\n",
    "        words = tf_idf_vectorizor.get_feature_names()\n",
    "        # Top words.\n",
    "        top = [(words[i], x_means[i]) for i in sorted]\n",
    "        df = pd.DataFrame(top, columns = ['features', 'score'])\n",
    "        top_words.append(df)        \n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# Get the 15 top words per cluster.\n",
    "top_words = cluster_top_words(tf_idf_array, kmeans_pred, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the outcome of this step using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top words per cluster.\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "x = np.arange(len(top_words[0]))\n",
    "\n",
    "for i, df in enumerate(top_words):\n",
    "    ax = fig.add_subplot(1, len(top_words), i+1)\n",
    "    ax.set_title(\"Cluster: \" + str(i+1), fontsize=16)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_xlabel('score')\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "    ax.set_yticks(np.arange(0, 15, 1))\n",
    "    ax.barh(x, df.score, align='center', color='#0a7ff2')\n",
    "    yticks = ax.set_yticklabels(df.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering\n",
    "\n",
    "In the following code, we use ``KNN`` to extract the distances between each point in the data set and its 5 nearest neighbors. Then we sort the distances in ascending values and create the plot to get the point of maximum curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Perform 5-nn to extract the distances. \n",
    "nn = NearestNeighbors(n_neighbors=5).fit(pcaComponents)\n",
    "distances, idx = nn.kneighbors(pcaComponents)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "\n",
    "# Find the knne point.\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=0, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "\n",
    "# Plot the knee.\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.title('Knee point', fontsize=18)\n",
    "plt.xlabel(\"Points\", fontsize=16)\n",
    "plt.ylabel(\"Distance\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(['data', 'knee/elbow'], fontsize=16)\n",
    "\n",
    "print(\"Knee point: \" + str(distances[knee.knee]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous result let's apply the clustering method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Perform the clustering.\n",
    "db = DBSCAN(eps=distances[knee.knee], min_samples=4, n_jobs=-1).fit(pcaComponents)\n",
    "# Get the cluster labels.\n",
    "labels = db.labels_\n",
    "# Count the total number of clusters.\n",
    "num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# Count the total number of noise points.\n",
    "num_noise = list(labels).count(-1)\n",
    "\n",
    "print('Number of clusters: %d' % num_clusters)\n",
    "print('Number of noise points: %d' % num_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the silhouette coefficient for our example and the distribution of the data points in each cluster follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the model results.\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pcaComponents, labels))\n",
    "\n",
    "# Get the sample counts in each cluster\n",
    "counts = np.bincount(labels[labels>=0])\n",
    "print (counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the clusters as we did with the K-means case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters.\n",
    "colors = ['blue', 'yellow', 'green', 'red', 'black' #'red', 'deeppink', 'olive', 'goldenrod', #'lightcyan', 'navy'\n",
    "]\n",
    "\n",
    "vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(pcaComponents[:,0], pcaComponents[:,1], c=vectorizer(labels))\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the dendrogram for our dataset using agglomerative clustering and ``ward`` as the linkage function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendro = hierarchy.dendrogram(hierarchy.linkage(pcaComponents, method='ward'))\n",
    "plt.xlabel(\"hypothesis index\", fontsize=16)\n",
    "plt.ylabel(\"Euclidean distance\", fontsize=16)\n",
    "# Cut at 1.5 to get 3 clusters.\n",
    "plt.axhline(y=1.5, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet, we create an agglomerative clustering model and predict the cluster for each hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Perform the clustering.\n",
    "agg = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "agg_model = agg.fit_predict(pcaComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we proceed to the extraction of the 15 top words per cluster and provide the visualization of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 15 top words per cluster.\n",
    "top_words = cluster_top_words(tf_idf_array, agg.labels_, 15)\n",
    "\n",
    "# Plot the top words per cluster.\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "x = np.arange(len(top_words[0]))\n",
    "\n",
    "for i, df in enumerate(top_words):\n",
    "    ax = fig.add_subplot(1, len(top_words), i+1)\n",
    "    ax.set_title(\"Cluster: \" + str(i+1), fontsize=16)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_xlabel('score')\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    \n",
    "    ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "    ax.set_yticks(np.arange(0, 15, 1))\n",
    "    ax.barh(x, df.score, align='center', color='#0a7ff2')\n",
    "    yticks = ax.set_yticklabels(df.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Techniques for Text \n",
    "&copy;2021&ndash;2022, Nikos Tsourakis, <nikos@tsourakis.net>, Packt Publications. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "34c3ec88db1a123a786d67d086f3ede88281b71e687e4350202a680e0c5fcbcd"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
