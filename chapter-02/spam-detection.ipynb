{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 2</u> - Detecting Spam Emails\n",
    "\n",
    "A `spam detector` is software that runs on the mail server or our local computer and checks the inbox to detect possible spam. As with traditional letterboxes, an inbox is a destination for electronic mail messages. Generally, any spam detector has unhindered access to this repository and can perform tens, hundreds, or even thousands of checks per day to decide whether an incoming email is spam or not. Fortunately, spam detection is a ubiquitous technology that filters out irrelevant and possibly dangerous electronic correspondence.\n",
    "\n",
    "In this exercise, we implement a spam detector from scratch and present various techniques related to natural language processing.\n",
    "\n",
    "We will be using a subset of the annotated email corpus found here: <https://spamassassin.apache.org/old/publiccorpus/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Windows users download and install \"Microsoft C++ Build Tools\" from: https://visualstudio.microsoft.com/visual-cpp-build-tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "print(installed_packages)\n",
    "required_packages = {'matplotlib', 'wordcloud', 'sklearn', 'nltk'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "For creating the spam detector, we can examine the message’s body and check whether certain words appear more often? Intuitively, those words can serve as a way to separate the two types of emails. An easy way to perform this task is to visualize the body of the message using `word clouds` (also known as `tag clouds`). With this visualization technique, recurring words in the dataset (excluding  articles, pronouns, and a few other cases) appear larger than infrequent ones.\n",
    "\n",
    "One possible implementation of word clouds in Python is the [word_cloud module](https://github.com/amueller/word_cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import the necessary modules.\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Read the text from the file spam.txt.\n",
    "text = open('./data/spam.txt').read()\n",
    "\n",
    "# Create and configure the word cloud object.\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000)\n",
    "\n",
    "# Generate the word cloud image from the text.\n",
    "wordcloud = wc.generate(text.lower())\n",
    "\n",
    "# Display the generated image.\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image suggests that the most common word in our spam message is *“virus”* (all words are lowercase).\n",
    "\n",
    "Notice that techniques such as word clouds are commonplace in ML problems to explore text data before resorting to any solution. We call this process `Exploratory Data Analysis` (EDA). EDA provides an understanding of where to direct our subsequent analysis and visualization methods are the primary tool for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding\n",
    "\n",
    "Next, we learn about the standard approaches for representing words in a piece of text data. In ML problems, there are various ways to represent words; `label encoding` is the simplest form. For example, consider this quote from Aristotle: *\"a friend to all is a friend to none\"*. Using the label-encoding scheme and a dictionary with words to indices. E.g., the word *\"friend\"* maps to number *2*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create the label encoder and fit it with data.\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit([\"a\", \"all\", \"friend\", \"is\", \"none\", \"to\"])\n",
    "\n",
    "# Transform an input sentence.\n",
    "x = labelencoder.transform([\"a\", \"friend\", \"to\", \"all\", \"is\", \"a\", \"friend\", \"to\", \"none\"])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Another well-known word representation technique is `one-hot encoding`, which codifies every word as a vector with zeros and a single one. Notice that the position of the one uniquely identifies a specific word; consequently, no two words exist with the same one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# The input.\n",
    "x = [['a'], ['friend'], ['to'], ['all'], ['is'], ['a'], ['friend'], ['to'], ['none']]\n",
    "\n",
    "# Create the one-hot encoder.\n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform.\n",
    "enc = onehotencoder.fit_transform(x).toarray()  \n",
    "print(enc.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token count encoding\n",
    "\n",
    "`Token count encoding`, also known as the `Bag-of-Words` (BoW) representation, counts the absolute frequency of each word within a sentence or a document. The input is represented as a bag of words without taking into account grammar or word order. This method uses a `Term Document Matrix` (TDM) matrix that describes the frequency of each term in the text. Notice that the corresponding cell in the table contains the value *\"0\"* when no such word is present in the quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The input.\n",
    "X = [\"a friend to all is a friend to none\"]\n",
    "\n",
    "# Create the count vectorizer.\n",
    "vectorizer = CountVectorizer(token_pattern='[a-zA-Z]+')\n",
    "\n",
    "# Fit and transform.\n",
    "x = vectorizer.fit_transform(X)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `bigrams` to do the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The input.\n",
    "X = [\"a friend to all is a friend to none\"]\n",
    "\n",
    "# Create the count vectorizer using bi-grams.\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), token_pattern='[a-zA-Z]+')\n",
    "\n",
    "# Fit and transform.\n",
    "x = vectorizer.fit_transform(X)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf encoding\n",
    "\n",
    "One limitation of BoW representations is that they do not consider the value of words inside the corpus. For example, if solely frequency were of prime importance, articles such as *\"a\"* or *\"the\"* would provide the most information for a document. Therefore, we need a representation that *penalizes* these frequent words. The remedy is the term `frequency-inverse document frequency` (tf-idf) encoding scheme that allows us to weigh each word in the text. You can consider tf-idf as a heuristic where more common words tend to be less relevant for most semantic classification tasks, and the weighting reflects this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a dummy corpus.\n",
    "corpus = ['We need to meet tomorrow at the cafeteria.',\n",
    "        'Meet me tomorrow at the cafeteria.',\n",
    "        'You have inherited millions of dollars.',\n",
    "        'Millions of dollars just for you.']\n",
    "\n",
    "# Create the tf-idf vectorizer.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf matrix.\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the result as an array.\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we calculate the `cosine similarity` of the tf-idf vectors between the corpus’s first and second examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert the matrix to an array.\n",
    "tfidf_array = tfidf.toarray()\n",
    "\n",
    "# Calculate the cosine similarity between the first amd second example.\n",
    "print(cosine_similarity([tfidf_array[0]], [tfidf_array[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the value between the first and second examples is high and equal to *0.62*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity among all examples.\n",
    "print(cosine_similarity(tfidf_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We also repeat the same calculation between all tf-idf vectors. Between the first and the third example, it is *0*, *0.55* between the third and the fourth, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "`Tokenization` is a general process where we split textual data into smaller components called tokens. These can be words, phrases, symbols, or other meaningful elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the toolkit.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the input text.\n",
    "wordTokens = nltk.word_tokenize(\"a friend to all is a friend to none\")\n",
    "\n",
    "print(wordTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As words are the tokens of a sentence, sentences are the tokens of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input paragraph.\n",
    "sentenceTokens = nltk.sent_tokenize(\"A friend to all is a friend to none. \\\n",
    "                                    A friend to none is a friend to all. \\\n",
    "                                    A friend is a friend.\")\n",
    "\n",
    "print(sentenceTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create our tokenizer by incorporating different search patterns called `regular expressions` (regexp). The code for tokenizing email addresses is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Regexp tokenizer.\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(pattern='^([a-z0-9_\\.-]+)@([a-z0-9_\\.-]+)\\.([a-z\\.]{2,6})$')\n",
    "\n",
    "# Tokenize a valid email address.\n",
    "tokens = tokenizer.tokenize(\"john@doe.com\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tokens for the invalid email are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a non-valid email address.\n",
    "tokens = tokenizer.tokenize(\"john-AT-doe.com\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal\n",
    "\n",
    "A typical task during the preprocessing phase is removing all the words that presumably help us focus on the most important information in the text. These are called `stop words` and there is no universal list in English or any other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Read the text from the file data.txt.\n",
    "text = open('./data/spam.txt').read()\n",
    "\n",
    "# Get all stopwords and update with few others.\n",
    "sw = set(STOPWORDS) \n",
    "sw.update([\"dear\", \"virus\", \"mr\"])\n",
    "\n",
    "# Create and configure the word cloud object.\n",
    "wc = WordCloud(background_color=\"white\", stopwords=sw, max_words=2000)\n",
    "\n",
    "# Generate the word cloud image from the text.\n",
    "wordcloud = wc.generate(text.lower())\n",
    "\n",
    "# Display the generated image.\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "`Stemming` maps a word to its root form and is the process of cutting off the end (suffix) or the beginning (prefix) of an inflected word and ending up with its stem (the root word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Porter stemmer.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem the words 'playing', 'plays', 'played'.\n",
    "stemmer.stem('playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('plays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of stemming doesn’t need to be a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the word 'bravery'\n",
    "stemmer.stem('bravery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create our stemmer using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Create the stemmer matching words ending with 'ed'.\n",
    "stemmer = RegexpStemmer('ed')\n",
    "\n",
    "# Stem the verbs 'playing', 'plays', 'played'.\n",
    "stemmer.stem('playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('plays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "`Lemmatization` is another sophisticated approach for reducing the inflectional forms of a word to a base root. The method performs morphological analysis of the word and obtains its proper lemma (the base form under which it appears in a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the WordNet Lemmatizer.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Create the lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the verb 'played'.\n",
    "lemmatizer.lemmatize('played', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the verb 'led'.\n",
    "lemmatizer.lemmatize('led', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the adjective 'better'.\n",
    "lemmatizer.lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The [SpamAssassin public mail corpus](https://spamassassin.apache.org/old/publiccorpus/) is a selection of email messages suitable for developing spam filtering systems. It \n",
    "offers two variants for the messages, either in plain text or HTML formatting. For simplicity, we will use only the first type in this exercise. The term coined to describe the opposite of spam emails is ham since the two words are related to meat products (spam refers to canned ham). In this exercise, we use a subset of the corpus. \n",
    "\n",
    "\n",
    "Initially, we read the messages for the two categories (ham and spam) and split them into training and testing groups. As a rule of thumb, we can choose a *75:25* split between the two sets, attributing a more significant proportion to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import glob\n",
    "import numpy as np\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the path for each email file for both categories.\n",
    "ham_files = train_test_split(glob.glob('./data/20030228_hard_ham/hard_ham/*'), random_state=123)\n",
    "spam_files = train_test_split(glob.glob('./data/20050311_spam_2/spam_2/*'), random_state=123)\n",
    "\n",
    "# Method for getting the content of an email.\n",
    "def get_content(filepath):\n",
    "    file = open(filepath, encoding='latin1')\n",
    "    message = email.message_from_file(file)\n",
    "    \n",
    "    for msg_part in message.walk():\n",
    "        # Keep only messages with text/plain content.\n",
    "        if msg_part.get_content_type() == 'text/plain':\n",
    "            return msg_part.get_payload()\n",
    "\n",
    "# Get the training and testing data.\n",
    "ham_train_data = [get_content(i) for i in ham_files[0]]\n",
    "ham_test_data = [get_content(i) for i in ham_files[1]]\n",
    "spam_train_data = [get_content(i) for i in spam_files[0]]\n",
    "spam_test_data = [get_content(i) for i in spam_files[1]]\n",
    "\n",
    "# Keep emails with non-empty content.\n",
    "ham_train_data = list(filter(None, ham_train_data))\n",
    "ham_test_data = list(filter(None, ham_test_data))\n",
    "spam_train_data = list(filter(None, spam_train_data))\n",
    "spam_test_data = list(filter(None, spam_test_data))\n",
    "\n",
    "# Merge the train/test files for both categories.\n",
    "train_data = np.concatenate((ham_train_data, spam_train_data))\n",
    "test_data = np.concatenate((ham_test_data, spam_test_data))\n",
    "\n",
    "# Assign a class for each email (ham = 0, spam = 1).\n",
    "ham_train_class = [0]*len(ham_train_data)\n",
    "ham_test_class = [0]*len(ham_test_data)\n",
    "spam_train_class = [1]*len(spam_train_data)\n",
    "spam_test_class = [1]*len(spam_test_data)\n",
    "\n",
    "# Merge the train/test classes for both categories.\n",
    "train_class = np.concatenate((ham_train_class, spam_train_class))\n",
    "test_class = np.concatenate((ham_test_class, spam_test_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to preprocess the data using the techniques we learned. In the code that follows, we remove all stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Tokenize the train/test data.\n",
    "train_data = [word_tokenize(i) for i in train_data]\n",
    "test_data = [word_tokenize(i) for i in test_data]\n",
    "\n",
    "\n",
    "# Method for removing the stop words.\n",
    "def remove_stop_words(input):\n",
    "    result = [i for i in input if i not in ENGLISH_STOP_WORDS]\n",
    "    return result\n",
    "\n",
    "# Remove the stop words.\n",
    "train_data = [remove_stop_words(i) for i in train_data]\n",
    "test_data = [remove_stop_words(i) for i in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we lemmatize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Method for lemmatizing the text.\n",
    "def lemmatize_text(input):\n",
    "    return [lemmatizer.lemmatize(i) for i in input]\n",
    "\n",
    "# Lemmatize the text.\n",
    "train_data = [lemmatize_text(i) for i in train_data]\n",
    "test_data = [lemmatize_text(i) for i in test_data]\n",
    "\n",
    "# Reconstruct the data.\n",
    "train_data = [\" \".join(i) for i in train_data]\n",
    "test_data = [\" \".join(i) for i in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the extraction of the features of each sentence in the two sets. This step uses tf-idf vectorization after training the vectorizer on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the vectorizer.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit with the train data.\n",
    "vectorizer.fit(train_data)\n",
    "\n",
    "# Transform the test/train data into features.\n",
    "train_data_features = vectorizer.transform(train_data)\n",
    "test_data_features = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the *670* emails in the training set is represented by a feature vector with a size of *28337*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "One of the most well-known supervised learning algorithms is the `Support Vector Machines` (SVM) algorithm.\n",
    "\n",
    "We evaluate its performance in both the test and training sets. We are primarily interested in the first result, as it quantifies the accuracy of our model on unseen data; essentially, how well it generalizes. On the other hand, the performance on the training set indicates how well our model learned from the training data. As you can observe, the accuracy, in the latter case, is not *100%* as one might expect. In practical problems, it is almost always much less than *99%*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create the classifier.\n",
    "svm_classifier = svm.SVC(kernel=\"rbf\", C=1.0, gamma=1.0, probability=True)\n",
    "\n",
    "# Fit the classifier with the train data.\n",
    "svm_classifier.fit(train_data_features.toarray(), train_class)\n",
    "\n",
    "# Get the classification score of the train data.\n",
    "svm_classifier.score(train_data_features.toarray(), train_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classification score of the test data.\n",
    "svm_classifier.score(test_data_features.toarray(), test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "\n",
    "`Naïve Bayes` is a classification algorithm based on the `Bayes Theorem`. \n",
    "\n",
    "s with the SVM, it is straightforward to incorporate Naïve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# Create the classifier.\n",
    "nb_classifier = naive_bayes.MultinomialNB(alpha=1.0)\n",
    "\n",
    "# Fit the classifier with the train data.\n",
    "nb_classifier.fit(train_data_features.toarray(), train_class)\n",
    "\n",
    "# Get the classification score of the train data.\n",
    "nb_classifier.score(train_data_features.toarray(), train_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we incorporate the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classification score of the test data.\n",
    "nb_classifier.score(test_data_features.toarray(), test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome suggests that the performance of this classifier is inferior. Also, notice the result on the actual training set, which is low and very close to the performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "The standard approach for any ML problem incorporates different classification algorithms and examines which works best. Previously, we used two classification methods for the spam filtering problem, but our job is not done yet; we need to evaluate their performance in more detail.\n",
    "\n",
    "`Accuracy` is the percentage of correctly classified examples by an algorithm divided by the total number of examples. The code below calculates accuracy on the same data; its value is identical to the one in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    " \n",
    "# Get the predicted classes.\n",
    "test_class_pred = nb_classifier.predict(test_data_features.toarray())\n",
    "\n",
    "# Calculate the accuracy on the test set.\n",
    "metrics.accuracy_score(test_class, test_class_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "`Precision` precision tells us the proportion of positive identifications that are, in reality, correct, and `Recall` tells us the proportion of the actual positives that are identified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision on the test set.\n",
    "metrics.precision_score(test_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the recall on the test set.\n",
    "metrics.recall_score(test_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-score\n",
    "\n",
    "`F-score`, is the harmonic mean of precision and recall. When precision and recall reach their perfect score (equal to *1*), F-score becomes *1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F-score on the test set.\n",
    "metrics.f1_score(test_class, test_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and AUC\n",
    "\n",
    "When the classifier returns some kind of confidence score for each prediction, we can use another technique for evaluating performance called the `Receiver Operator Characteristic` (ROC) curve. A ROC curve is a graphical plot that shows the model’s performance at all classification thresholds. The grayed area in these plots, called \n",
    "the `Area Under the ROC Curve` (AUC), is related to the quality of our model; the higher its surface, the better it is.\n",
    "\n",
    "Next, we calculate the ROC curves for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot the ROC curves.\n",
    "nb_disp = metrics.plot_roc_curve(nb_classifier, test_data_features.toarray(), test_class)\n",
    "svm_disp = metrics.plot_roc_curve(svm_classifier, test_data_features.toarray(), test_class, ax=nb_disp.ax_)\n",
    "svm_disp.figure_.suptitle(\"ROC curve comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the figure, the AUC is *0.98* for SVM and *0.87* for Naïve Bayes. All results so far corroborate our initial assumption of the superiority of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the ROC along with the `Precision-Recall curve` for the Naïve Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the scores for each prediction.\n",
    "probs = nb_classifier.predict_proba(test_data_features.toarray())\n",
    "test_score = probs[:, 1]\n",
    "\n",
    "# Compute the Receiver Operating Characteristic.\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_class, test_score)\n",
    "\n",
    "# Compute Area Under the Curve.\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Create the ROC curve.\n",
    "rc_display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MultinomialNB')\n",
    "\n",
    "# Create the precision recall curves.\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(test_class, test_score)\n",
    "ap = metrics.average_precision_score(test_class, test_score)\n",
    "pr_display = metrics.PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=ap, estimator_name='MultinomialNB')\n",
    "\n",
    "# Plot the curves.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "rc_display.plot(ax=ax1)\n",
    "pr_display.plot(ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both plots summarize the trade-offs between the rates on the x and y axes using different probability thresholds. In the right plot, the average precision (AP) is *0.97* for the Naïve Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have learned so far …\n",
    "\n",
    "* **Exploratory data analysis**\n",
    "    * Word clouds\n",
    "* **Text preprocessing**\n",
    "    * Tokenization\n",
    "    * Stop words removal\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "    * Regular expressions\n",
    "* **Text representations**\n",
    "    * Label encoding\n",
    "    * One-hot encoding\n",
    "    * Token count encoding\n",
    "    * Tf-idf encoding\n",
    "* **Classification**\n",
    "    * Supervised learning\n",
    "    * Creating train and test sets\n",
    "    * Feature engineering\n",
    "    * Overfitting\n",
    "* **Classification algorithms**\n",
    "    * Support Vector Machines\n",
    "    * Naïve Bayes\n",
    "* **Performance**\n",
    "    * Accuracy\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F-score\n",
    "    * ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Techniques for Text \n",
    "&copy;2021&ndash;2022, Nikos Tsourakis, <nikos@tsourakis.net>, Packt Publications. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ch01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8252dddc1dca896fa2fdae5e3bef3be6035ffc39302496fdf7f078527082c0a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
