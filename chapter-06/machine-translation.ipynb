{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 6</u>: Teaching Machines to Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "required_packages = {'nltk'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenize the input text.\n",
    "text = nltk.word_tokenize(\"The sky is blue\")\n",
    "\n",
    "# Parse the input.\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grammar that consists of six rules. \n",
    "# S:sentence, NP:noun phrase, DT:determiner, NN:noun, \n",
    "# VBZ:verb in the third person singular, JJ:adjective.\n",
    "analysis_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VBZ JJ\t\n",
    "    NP -> DT NN\t\n",
    "    DT -> 'The'\t\n",
    "    NN -> 'sky'\t\n",
    "    VBZ -> 'is'\t\n",
    "    JJ -> 'blue'\n",
    "    \"\"\")\n",
    " \t\n",
    "# Create the input.\n",
    "input = ['The', 'sky', 'is', 'blue']\n",
    "\n",
    "# Parse the input.\n",
    "parser = nltk.ChartParser(analysis_grammar)\n",
    "\n",
    "# Print the parse trees.\n",
    "for tree in parser.parse(input):\n",
    "    print(tree)\n",
    "    #tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grammar consists  of six but more powerful rules.\n",
    "analysis_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VBZ JJ\t\n",
    "    NP -> DT NN\t\n",
    "    DT -> 'The' | 'the'\t\n",
    "    NN -> 'sky' | 'sea'\t\n",
    "    VBZ -> 'is'\t\n",
    "    JJ -> 'blue' | 'red'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate\n",
    "\n",
    "# Generate ten examples at most.\n",
    "for sentence in generate(analysis_grammar, n=10):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dependency grammar that includes three rules.\n",
    "dependency_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "    'is' -> 'sky' | 'sea' | 'blue' | 'red'\n",
    "    'sky' -> 'The' | 'the' \n",
    "    'sea' -> 'The' | 'the' \n",
    "    \"\"\")\n",
    "\n",
    "# Create the dependency parser.\n",
    "pdp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "\n",
    "# Create the input.\n",
    "input = ['The', 'sky', 'is', 'blue']\n",
    "\n",
    "# Parse the input.\n",
    "trees = pdp.parse(input)\n",
    "\n",
    "# Print the parse trees.\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk models/corpora.\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Tokenize the input text.\n",
    "text = nltk.word_tokenize(\"The Aston Martin is blue\")\n",
    "\n",
    "# Parse the input.\n",
    "tags = nltk.pos_tag(text)\n",
    "\n",
    "# Find the name entities.\n",
    "tree = nltk.ne_chunk(tags)\n",
    "\n",
    "# Draw the tree.\n",
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IOB tags.\n",
    "iob_tags = nltk.tree2conlltags(tree)\n",
    "\n",
    "# Print the IOB tags.\n",
    "print(iob_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grammar string.\n",
    "g = \"\"\"\n",
    "\n",
    "# S expansion productions.\n",
    "S[AGR1=?np, ARG2=?vbz, ARG3=?jj] -> NP[AGR=?np] VBZ[AGR=?vbz] JJ[AGR=?jj]\n",
    "\n",
    "# NP expansion productions.\n",
    "NP[AGR=[DT=?dt, NN=?nn]] -> DT[AGR=?dt] NN[AGR=?nn] \n",
    "\n",
    "# Lexical productions.\n",
    "DT[AGR=[TEXT='Le', SEM='determiner']] -> 'The' \n",
    "DT[AGR=[TEXT='le', SEM='determiner']] -> 'the' \n",
    "NN[AGR=[TEXT='ciel', SEM='noun']] -> 'sky'\n",
    "NN[AGR=[TEXT='mer', SEM='noun']] -> 'sea'\n",
    "VBZ[AGR=[TEXT='être', SEM='verb', TENSE='present', NUM='singular']] -> 'is'\n",
    "JJ[AGR=[TEXT='bleu', SEM='adjective']] -> 'blue'\n",
    "JJ[AGR=[TEXT='rouge', SEM='adjective']] -> 'red'\n",
    "\"\"\"\n",
    "\n",
    "# Create the input, transfer grammar, and parser.\n",
    "input = ['The', 'sky', 'is', 'blue']\n",
    "transfer_grammar = nltk.grammar.FeatureGrammar.fromstring(g)\n",
    "parser = nltk.parse.FeatureEarleyChartParser(transfer_grammar)\n",
    "\n",
    "# Parse the input and print the result.\n",
    "trees = parser.parse(input)\n",
    "for tree in trees: print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grammar string.\n",
    "g = \"\"\"\n",
    "\n",
    "# S expansion productions.\n",
    "S[AGR1=?np, ARG2=?vbz, ARG3=?jj] -> NP[AGR=?np] VBZ[AGR=?vbz] JJ[AGR=?jj]\n",
    "\n",
    "# NP expansion productions.\n",
    "NP[AGR=[DT=?dt, NN=?nn]] -> DT[AGR=?dt] NN[AGR=?nn] \n",
    "\n",
    "# Lexical productions.\t\n",
    "DT[AGR=[TEXT='Le']] -> 'Le' \n",
    "DT[AGR=[TEXT='le']] -> 'le' \n",
    "NN[AGR=[TEXT='ciel']] -> 'ciel'\n",
    "NN[AGR=[TEXT='mer']] -> 'mer'\n",
    "VBZ[AGR=[TEXT='est', SEM='verb', TENSE='present', NUM='singular']] -> 'être'\n",
    "JJ[AGR=[TEXT='bleu']] -> 'bleu'\n",
    "JJ[AGR=[TEXT='rouge']] -> 'rouge'\n",
    "\"\"\"\n",
    "\n",
    "# Create the input, transfer grammar, and parser.\n",
    "input = ['Le', 'ciel', 'être', 'bleu']\n",
    "generation_grammar = nltk.grammar.FeatureGrammar.fromstring(g)\n",
    "parser = nltk.parse.FeatureEarleyChartParser(generation_grammar)\n",
    "\n",
    "# Parse the input and print the result.\n",
    "trees = parser.parse(input)\n",
    "for tree in trees: print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "# Hold the bi-lingual text.\n",
    "bitext = []\n",
    "\n",
    "# Create two examples from German to English, along with the alignments.\n",
    "bitext.append(AlignedSent(['blue', 'is', 'The', 'sky'], \n",
    "                            ['Le', 'ciel', 'est', 'bleu'], \n",
    "                            Alignment.fromstring('0-3 1-2 2-0 3-1')))\n",
    "bitext.append(AlignedSent(['yellow', 'is', 'The', 'sun'], \n",
    "                            ['Le', 'soleil', 'est', 'jaune'], \n",
    "                            Alignment.fromstring('0-3 1-2 2-0 3-1')))\n",
    "\n",
    "# Print the source words in the second example.\n",
    "bitext[1].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target words in the second example.\n",
    "bitext[1].mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the alignments in the second example.\n",
    "bitext[1].alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk corpus.\n",
    "nltk.download('comtrans')\n",
    "\n",
    "from nltk.corpus import comtrans\n",
    "\n",
    "# Get the first example from the english/french corpus.\n",
    "fe = comtrans.aligned_sents('alignment-en-fr.txt')[0]\n",
    "\n",
    "# Print the source words.\n",
    "fe.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target words.\n",
    "fe.mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the alignments.\n",
    "fe.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 52nd example from the English/French corpus.\n",
    "fe = comtrans.aligned_sents('alignment-en-fr.txt')[52]\n",
    "\n",
    "# Print the source words.\n",
    "fe.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target words.\n",
    "fe.mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the alignments.\n",
    "fe.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.ibm2\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "# Hold the bi-lingual text.\n",
    "bitext = []\n",
    "\n",
    "# Create examples from French to English.\n",
    "bitext.append(AlignedSent(\n",
    "    ['petite', 'est', 'la', 'maison'],\n",
    "    ['the', 'house', 'is', 'small']))\n",
    "bitext.append(AlignedSent(\n",
    "    ['la', 'maison', 'est', 'grande'], \n",
    "    ['the', 'house', 'is', 'big']))\n",
    "bitext.append(AlignedSent\n",
    "    (['le', 'livre', 'est', 'petit'], \n",
    "    ['the', 'book', 'is', 'small']))\n",
    "bitext.append(AlignedSent(\n",
    "    ['la', 'maison'], ['the', 'house']))\n",
    "bitext.append(AlignedSent(['le', 'livre'], ['the', 'book']))\n",
    "bitext.append(AlignedSent(['un', 'livre'], ['a', 'book']))\n",
    "\n",
    "# Create the lexical translation model from the examples.\n",
    "ibm2 = nltk.translate.ibm2.IBMModel2(bitext, 5)\n",
    "\n",
    "# Get the translation probabilities from the model.\n",
    "print(round(ibm2.translation_table['livre']['book'], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider one example from the bi-lingual text.\n",
    "test_sentence = bitext[2]\n",
    "test_sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence.mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log\n",
    "from nltk.translate import PhraseTable\n",
    "from nltk.translate.stack_decoder import StackDecoder\n",
    "\n",
    "# Create the phrase table.\n",
    "phrase_table = PhraseTable()\n",
    "\n",
    "# Populate the table with examples.\n",
    "phrase_table.add(('das',), ('the', 'it'), log(0.4))\n",
    "phrase_table.add(('das', 'ist'), ('this', 'is'), log(0.8))\n",
    "phrase_table.add(('ein',), ('a',), log(0.8))\n",
    "phrase_table.add(('haus',), ('house',), log(1.0))\n",
    "phrase_table.add(('!',), ('!',), log(0.8))\n",
    "\n",
    "# Create the dictionary of probabilities for each n-gram.\n",
    "language_prob = defaultdict(lambda: -999.0)\n",
    "\n",
    "# Populate the dictionary uni-grams and bi-grams.\n",
    "language_prob[('this',)] = log(0.8)\n",
    "language_prob[('is',)] = log(0.6)\n",
    "language_prob[('a', 'house')] = log(0.2)\n",
    "language_prob[('!',)] = log(0.1)\n",
    "\n",
    "# Create the language model.\n",
    "language_model = type('',(object,),{'probability_change': lambda self, context, phrase: language_prob[phrase], 'probability': lambda self, phrase: language_prob[phrase]})()\n",
    "\n",
    "# Create the stack decoder and translate a sentence.\n",
    "stack_decoder = StackDecoder(phrase_table, language_model)\n",
    "stack_decoder.translate(['das', 'ist', 'ein', 'haus', '!'])\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from nltk.translate import PhraseTable\n",
    "\n",
    "# The translation model.\n",
    "phrase_table = PhraseTable()\n",
    "\t\n",
    "# Open the phrase-table file.\n",
    "f = open(\"./data/phrase-table\", \"r\")\n",
    "\n",
    "# Iterate over all lines.\n",
    "for line in f:\n",
    "    \n",
    "    # Extract all the elements in the line.\n",
    "    line = line.strip().split(' ||| ')\n",
    "    \n",
    "    # Get the elements we are interested in.\n",
    "    source, target, probabilities = line[:3]\n",
    "    \n",
    "    # Get the φ(f|e).\n",
    "    prob = float(probabilities.split( )[0])\n",
    "\t\n",
    "    # Store the information into our phrase_table.\n",
    "    phrase_table.add((source,), (target,), log(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# The probability of each n-gram. \n",
    "language_prob = defaultdict(lambda: -999.0)\n",
    "\n",
    "# Open the europarl language model.\n",
    "with gzip.open('./data/europarl.srilm.gz', 'r') as f:\n",
    "\t\n",
    "    # Iterate over all lines.\n",
    "    for line in f:\n",
    "\t\n",
    "        # Use tab to split each line.\n",
    "        line = line.decode('latin-1').strip().split('\\t')\n",
    "\n",
    "        # There is enough info in the line.\n",
    "        if len(line) > 1:\n",
    "            prob, ngram = line[:2]\n",
    "            language_prob[(ngram,)] = float(prob)\n",
    "\n",
    "# Create our language model.  \t\t\n",
    "language_model = type('',(object,),{'probability_change': lambda self, context, phrase: language_prob[phrase], 'probability': lambda self, phrase: language_prob[phrase]})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.stack_decoder import StackDecoder\n",
    "\n",
    "# Translate a German sentence to English.\n",
    "stack_decoder = StackDecoder(phrase_table, language_model)\n",
    "stack_decoder.translate(['das', 'haus', 'ist', 'klein'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "5ec7f24bccdc5982815fc9e679db841794fd8b692b70ffe03fd828d6ea20f1fe"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
