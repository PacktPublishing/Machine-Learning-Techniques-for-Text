{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 6</u>: Teaching Machines to Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A serious impediment to spreading new information, ideas, and knowledge is the language barriers imposed by the different languages spoken worldwide. Despite the cultural richness brought to our global heritage, they can pose significant hurdles to efficient human communication. This exercice focuses on `machine translation` (MT), which aims to alleviate these barriers. MT is the process of automatically converting a piece of text from a source into a target language without human intervention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "required_packages = {'nltk'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, download the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the data directory already exists.\n",
    "if not os.path.exists(\"data\"):\n",
    "    # URL of the zip data file to download.\n",
    "    url = \"https://github.com/PacktPublishing/Machine-Learning-Techniques-for-Text/raw/main/chapter-06/data.zip\"\n",
    "\n",
    "    # If it doesn't exist, download the zip file.\n",
    "    !wget {url}\n",
    "\n",
    "    # Unzip the file into the \"data\" folder.\n",
    "    !unzip -q \"data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based machine translation\n",
    "\n",
    "We will begin our journey of MT with the classical approach, known as `rule-based machine translation` (RBMT), which aims to exploit linguistic information about the source and target languages. RBMT techniques fall under the broad category of `knowledge-based systems`, which mainly aim to capture the knowledge of human experts to solve complex problems.\n",
    "\n",
    "First, we will incorporate _nltk_ to perform `POS tagging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('blue', 'JJ')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Tokenize the input text.\n",
    "text = nltk.word_tokenize(\"The sky is blue\")\n",
    "\n",
    "# Parse the input.\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a _CFG_ (named _analysis_grammar_) that consists of six rules signified with the _->_ symbol. Then we parse the input phrase using the analysis grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DT The) (NN sky)) (VBZ is) (JJ blue))\n"
     ]
    }
   ],
   "source": [
    "# Create the grammar that consists of six rules. \n",
    "# S:sentence, NP:noun phrase, DT:determiner, NN:noun, \n",
    "# VBZ:verb in the third person singular, JJ:adjective.\n",
    "analysis_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VBZ JJ\t\n",
    "    NP -> DT NN\t\n",
    "    DT -> 'The'\t\n",
    "    NN -> 'sky'\t\n",
    "    VBZ -> 'is'\t\n",
    "    JJ -> 'blue'\n",
    "    \"\"\")\n",
    " \t\n",
    "# Create the input.\n",
    "input = ['The', 'sky', 'is', 'blue']\n",
    "\n",
    "# Parse the input.\n",
    "parser = nltk.ChartParser(analysis_grammar)\n",
    "\n",
    "# Print the parse trees.\n",
    "for tree in parser.parse(input):\n",
    "    print(tree)\n",
    "    #tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the extended grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grammar consists  of six but more powerful rules.\n",
    "analysis_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VBZ JJ\t\n",
    "    NP -> DT NN\t\n",
    "    DT -> 'The' | 'the'\t\n",
    "    NN -> 'sky' | 'sea'\t\n",
    "    VBZ -> 'is'\t\n",
    "    JJ -> 'blue' | 'red'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that it supports all the necessary phrases, let’s generate 10 (_n=10_) possible expansions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue\n",
      "The sky is red\n",
      "The sea is blue\n",
      "The sea is red\n",
      "the sky is blue\n",
      "the sky is red\n",
      "the sea is blue\n",
      "the sea is red\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.generate import generate\n",
    "\n",
    "# Generate ten examples at most.\n",
    "for sentence in generate(analysis_grammar, n=10):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a grammar that consists of three rules for word-to-word dependency relations and incorporate it through a dependency parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(is (sky The) blue)\n"
     ]
    }
   ],
   "source": [
    "# Create the dependency grammar that includes three rules.\n",
    "dependency_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "    'is' -> 'sky' | 'sea' | 'blue' | 'red'\n",
    "    'sky' -> 'The' | 'the' \n",
    "    'sea' -> 'The' | 'the' \n",
    "    \"\"\")\n",
    "\n",
    "# Create the dependency parser.\n",
    "pdp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "\n",
    "# Create the input.\n",
    "input = ['The', 'sky', 'is', 'blue']\n",
    "\n",
    "# Parse the input.\n",
    "trees = pdp.parse(input)\n",
    "\n",
    "# Print the parse trees.\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s learn how to perform `NER` in _nltk_ while using the phrase _The Aston Martin_ is blue as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\tsouraki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tsouraki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download nltk models/corpora.\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Tokenize the input text.\n",
    "text = nltk.word_tokenize(\"The Aston Martin is blue\")\n",
    "\n",
    "# Parse the input.\n",
    "tags = nltk.pos_tag(text)\n",
    "\n",
    "# Find the name entities.\n",
    "tree = nltk.ne_chunk(tags)\n",
    "\n",
    "# Draw the tree.\n",
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must extract the tagging tokens using the `IOB format` (short for inside, outside, beginning), which is used for tagging tokens in a chunking task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT', 'O'), ('Aston', 'NNP', 'B-ORGANIZATION'), ('Martin', 'NNP', 'I-ORGANIZATION'), ('is', 'VBZ', 'O'), ('blue', 'JJ', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Get the IOB tags.\n",
    "iob_tags = nltk.tree2conlltags(tree)\n",
    "\n",
    "# Print the IOB tags.\n",
    "print(iob_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IOB provides three tags to refer to parts of a chunk (group of words).\n",
    "\n",
    "In the following code, we start with the definition of our feature grammar and use it to parse an input phrase. The grammar includes several attribute-value pairs; for example, the _SEM_ attribute can have _noun_ as its value. With the aid of the transfer rules, we can parse a source representation (in English) and return a sequence of attributes named _TEXT_ with the target language representations (in French). Observe the hierarchical expansion of the rules as the sentence, _S_, consists of noun phrases, _NP_, which consists of nouns, _NN_, and determiners, _DT_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[AGR1=[DT=[SEM='determiner', TEXT='Le'], NN=[SEM='noun', TEXT='ciel']], ARG2=[NUM='singular', SEM='verb', TENSE='present', TEXT='être'], ARG3=[SEM='adjective', TEXT='bleu']]\n",
      "  (NP[AGR=[DT=[SEM='determiner', TEXT='Le'], NN=[SEM='noun', TEXT='ciel']]]\n",
      "    (DT[AGR=[SEM='determiner', TEXT='Le']] The)\n",
      "    (NN[AGR=[SEM='noun', TEXT='ciel']] sky))\n",
      "  (VBZ[AGR=[NUM='singular', SEM='verb', TENSE='present', TEXT='être']]\n",
      "    is)\n",
      "  (JJ[AGR=[SEM='adjective', TEXT='bleu']] blue))\n"
     ]
    }
   ],
   "source": [
    "# Create the grammar string.\n",
    "g = \"\"\"\n",
    "\n",
    "# S expansion productions.\n",
    "S[AGR1=?np, ARG2=?vbz, ARG3=?jj] -> NP[AGR=?np] VBZ[AGR=?vbz] JJ[AGR=?jj]\n",
    "\n",
    "# NP expansion productions.\n",
    "NP[AGR=[DT=?dt, NN=?nn]] -> DT[AGR=?dt] NN[AGR=?nn] \n",
    "\n",
    "# Lexical productions.\n",
    "DT[AGR=[TEXT='Le', SEM='determiner']] -> 'The' \n",
    "DT[AGR=[TEXT='le', SEM='determiner']] -> 'the' \n",
    "NN[AGR=[TEXT='ciel', SEM='noun']] -> 'sky'\n",
    "NN[AGR=[TEXT='mer', SEM='noun']] -> 'sea'\n",
    "VBZ[AGR=[TEXT='être', SEM='verb', TENSE='present', NUM='singular']] -> 'is'\n",
    "JJ[AGR=[TEXT='bleu', SEM='adjective']] -> 'blue'\n",
    "JJ[AGR=[TEXT='rouge', SEM='adjective']] -> 'red'\n",
    "\"\"\"\n",
    "\n",
    "# Create the input, transfer grammar, and parser.\n",
    "input = ['The', 'sky', 'is', 'blue']\n",
    "transfer_grammar = nltk.grammar.FeatureGrammar.fromstring(g)\n",
    "parser = nltk.parse.FeatureEarleyChartParser(transfer_grammar)\n",
    "\n",
    "# Parse the input and print the result.\n",
    "trees = parser.parse(input)\n",
    "for tree in trees: print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous output, we managed to obtain an internal representation in the target language.\n",
    "\n",
    "Let's now create the generation grammar, create the parser and test it with the representation we encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[AGR1=[DT=[TEXT='Le'], NN=[TEXT='ciel']], ARG2=[NUM='singular', SEM='verb', TENSE='present', TEXT='est'], ARG3=[TEXT='bleu']]\n",
      "  (NP[AGR=[DT=[TEXT='Le'], NN=[TEXT='ciel']]]\n",
      "    (DT[AGR=[TEXT='Le']] Le)\n",
      "    (NN[AGR=[TEXT='ciel']] ciel))\n",
      "  (VBZ[AGR=[NUM='singular', SEM='verb', TENSE='present', TEXT='est']]\n",
      "    être)\n",
      "  (JJ[AGR=[TEXT='bleu']] bleu))\n"
     ]
    }
   ],
   "source": [
    "# Create the grammar string.\n",
    "g = \"\"\"\n",
    "\n",
    "# S expansion productions.\n",
    "S[AGR1=?np, ARG2=?vbz, ARG3=?jj] -> NP[AGR=?np] VBZ[AGR=?vbz] JJ[AGR=?jj]\n",
    "\n",
    "# NP expansion productions.\n",
    "NP[AGR=[DT=?dt, NN=?nn]] -> DT[AGR=?dt] NN[AGR=?nn] \n",
    "\n",
    "# Lexical productions.\t\n",
    "DT[AGR=[TEXT='Le']] -> 'Le' \n",
    "DT[AGR=[TEXT='le']] -> 'le' \n",
    "NN[AGR=[TEXT='ciel']] -> 'ciel'\n",
    "NN[AGR=[TEXT='mer']] -> 'mer'\n",
    "VBZ[AGR=[TEXT='est', SEM='verb', TENSE='present', NUM='singular']] -> 'être'\n",
    "JJ[AGR=[TEXT='bleu']] -> 'bleu'\n",
    "JJ[AGR=[TEXT='rouge']] -> 'rouge'\n",
    "\"\"\"\n",
    "\n",
    "# Create the input, transfer grammar, and parser.\n",
    "input = ['Le', 'ciel', 'être', 'bleu']\n",
    "generation_grammar = nltk.grammar.FeatureGrammar.fromstring(g)\n",
    "parser = nltk.parse.FeatureEarleyChartParser(generation_grammar)\n",
    "\n",
    "# Parse the input and print the result.\n",
    "trees = parser.parse(input)\n",
    "for tree in trees: print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example-based machine translation\n",
    "\n",
    "The reliance on linguistic rules presents many shortcomings. As we saw previously, using a corpus of already-translated examples could serve as a model to base the translation task on. This is the basic idea behind `example-based machine translation` (EBMT) systems; keep track of well-translated fragments and use this information to facilitate the translation of new sentences. \n",
    "\n",
    "Let’s learn how to use the alignments that have been defined for a bilingual pair programmatically. In the following code, we are considering two examples from the English-to-French pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow', 'is', 'The', 'sun']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "# Hold the bi-lingual text.\n",
    "bitext = []\n",
    "\n",
    "# Create two examples from German to English, along with the alignments.\n",
    "bitext.append(AlignedSent(['blue', 'is', 'The', 'sky'], \n",
    "                            ['Le', 'ciel', 'est', 'bleu'], \n",
    "                            Alignment.fromstring('0-3 1-2 2-0 3-1')))\n",
    "bitext.append(AlignedSent(['yellow', 'is', 'The', 'sun'], \n",
    "                            ['Le', 'soleil', 'est', 'jaune'], \n",
    "                            Alignment.fromstring('0-3 1-2 2-0 3-1')))\n",
    "\n",
    "# Print the source words in the second example.\n",
    "bitext[1].words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the word _yellow_ at position _0_ is aligned with the word _jaune_ at position _3_. \n",
    "\n",
    "We can verify these alignments using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Le', 'soleil', 'est', 'jaune']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the target words in the second example.\n",
    "bitext[1].mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 3), (1, 2), (2, 0), (3, 1)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the alignments in the second example.\n",
    "bitext[1].alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load the _comtrans_ module and pick the first example from the English-to-French dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to\n",
      "[nltk_data]     C:\\Users\\tsouraki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Resumption', 'of', 'the', 'session']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk corpus.\n",
    "nltk.download('comtrans')\n",
    "\n",
    "from nltk.corpus import comtrans\n",
    "\n",
    "# Get the first example from the english/french corpus.\n",
    "fe = comtrans.aligned_sents('alignment-en-fr.txt')[0]\n",
    "\n",
    "# Print the source words.\n",
    "fe.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target words in this case are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reprise', 'de', 'la', 'session']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the target words.\n",
    "fe.mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract the alignments between the source and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 0), (1, 1), (2, 2), (3, 3)])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the alignments.\n",
    "fe.alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, the mapping of the words is one-to-one. Unfortunately, this is not the case in most MT tasks. Consider, for example, the following pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'do', 'not', 'know', 'what', 'is', 'happening', '.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the 52nd example from the English/French corpus.\n",
    "fe = comtrans.aligned_sents('alignment-en-fr.txt')[52]\n",
    "\n",
    "# Print the source words.\n",
    "fe.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nous', 'ne', 'savons', 'pas', 'ce', 'qui', 'se', 'passe', '.']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the target words.\n",
    "fe.mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output in this case is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 0), (1, 1), (2, 3), (3, 2), (4, 4), (4, 5), (5, 6), (6, 7), (7, 8)])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the alignments.\n",
    "fe.alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates a few bilingual pairs from French to English that can be used to train a lexical translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.ibm2\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "# Hold the bi-lingual text.\n",
    "bitext = []\n",
    "\n",
    "# Create examples from French to English.\n",
    "bitext.append(AlignedSent(\n",
    "    ['petite', 'est', 'la', 'maison'],\n",
    "    ['the', 'house', 'is', 'small']))\n",
    "bitext.append(AlignedSent(\n",
    "    ['la', 'maison', 'est', 'grande'], \n",
    "    ['the', 'house', 'is', 'big']))\n",
    "bitext.append(AlignedSent\n",
    "    (['le', 'livre', 'est', 'petit'], \n",
    "    ['the', 'book', 'is', 'small']))\n",
    "bitext.append(AlignedSent(\n",
    "    ['la', 'maison'], ['the', 'house']))\n",
    "bitext.append(AlignedSent(['le', 'livre'], ['the', 'book']))\n",
    "bitext.append(AlignedSent(['un', 'livre'], ['a', 'book']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous examples, we can create a model and examine the probability of the word _livre_ being translated as _book_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879\n"
     ]
    }
   ],
   "source": [
    "# Create the lexical translation model from the examples.\n",
    "ibm2 = nltk.translate.ibm2.IBMModel2(bitext, 5)\n",
    "\n",
    "# Get the translation probabilities from the model.\n",
    "print(round(ibm2.translation_table['livre']['book'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t be surprised that the output probability is not equal to 1.0. All models suff er from certain limitations such as biases, vagaries of data noise and sampling, and so forth. Comparing _livre_ with any other word in the example gives a much smaller probability. Finally, we can obtain the alignments for one sample phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le', 'livre', 'est', 'petit']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider one example from the bi-lingual text.\n",
    "test_sentence = bitext[2]\n",
    "test_sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'book', 'is', 'small']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence.mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 0), (1, 1), (2, 2), (3, 3)])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence.alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BMT techniques follow a top-down approach, and domain experts are required to create models that can replicate the data. Conversely, data-driven approaches are bottom-up, and the data derives the model. Th is section focuses on `statistical machine translation` (SMT), which involves exploiting models whose parameters are learned from bilingual text \n",
    "corpora. They work on the assumption that every sentence in one language can be translated into any sentence in the target one. The overarching goal is to find the most probable translation in each case.\n",
    "\n",
    "First, to create the translation model, we use a phrase table that includes sequences of words in the source and target languages, along with their probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log\n",
    "from nltk.translate import PhraseTable\n",
    "from nltk.translate.stack_decoder import StackDecoder\n",
    "\n",
    "# Create the phrase table.\n",
    "phrase_table = PhraseTable()\n",
    "\n",
    "# Populate the table with examples.\n",
    "phrase_table.add(('das',), ('the', 'it'), log(0.4))\n",
    "phrase_table.add(('das', 'ist'), ('this', 'is'), log(0.8))\n",
    "phrase_table.add(('ein',), ('a',), log(0.8))\n",
    "phrase_table.add(('haus',), ('house',), log(1.0))\n",
    "phrase_table.add(('!',), ('!',), log(0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s create the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary of probabilities for each n-gram.\n",
    "language_prob = defaultdict(lambda: -999.0)\n",
    "\n",
    "# Populate the dictionary uni-grams and bi-grams.\n",
    "language_prob[('this',)] = log(0.8)\n",
    "language_prob[('is',)] = log(0.6)\n",
    "language_prob[('a', 'house')] = log(0.2)\n",
    "language_prob[('!',)] = log(0.1)\n",
    "\n",
    "# Create the language model.\n",
    "language_model = type('',(object,),{'probability_change': lambda self, context, phrase: language_prob[phrase], 'probability': lambda self, phrase: language_prob[phrase]})()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stack decoder utilizes the two models to extract the translation of the German phrase _das ist ein haus_ (formally, nouns in German should be capitalized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'house', '!']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the stack decoder and translate a sentence.\n",
    "stack_decoder = StackDecoder(phrase_table, language_model)\n",
    "stack_decoder.translate(['das', 'ist', 'ein', 'haus', '!'])\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have learned …\n",
    "\n",
    "| | |\n",
    "| --- | --- |\n",
    "| **Text preprocessing**<ul><li>Part-of-speech tagging</li><li>Parse trees</li><li>Name Entity Resolution</li></ul> | **ML algorithms & models**<ul><li>Rule-based MT</li><li>Example-based MT</li><li>Statistical MT</li></ul> |\n",
    "| | |"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "5ec7f24bccdc5982815fc9e679db841794fd8b692b70ffe03fd828d6ea20f1fe"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
