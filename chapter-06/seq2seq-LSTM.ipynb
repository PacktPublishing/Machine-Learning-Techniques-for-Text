{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 6</u>: Teaching Machines to Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A serious impediment to spreading new information, ideas, and knowledge is the language barriers imposed by the different languages spoken worldwide. Despite the cultural richness brought to our global heritage, they can pose significant hurdles to efficient human communication. This exercice focuses on `machine translation` (MT), which aims to alleviate these barriers. MT is the process of automatically converting a piece of text from a source into a target language without human intervention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "required_packages = {'numpy', 'tensorflow', 'pandas', 'keras'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence learning\n",
    "\n",
    "Many kinds of problems in machine learning involve transforming an input sequence into an output one. `Sequence-to-sequence` (seq2seq) learning has proven useful in applications that demand this transformation. Seq2seq, pronounced as `seek-to-seek`, learning falls under the category of neural MT, and unlike solutions based on RBMT and SMT, no domain knowledge of the languages involved is necessary. You can treat the translation problem as the association between input and output tokens of words or characters. Moreover, the translation is end-to-end, which means that one model is required instead of many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the seq2seq model, we will utilize an English-to-French bilingual corpus consisting of ~200K source-target pairs. Due to resource limitations, we will only keep _8000_ of those pairs to build the model. The code that follows shows the specific steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the first 8K pairs in the dataset.\n",
    "# Reduce the nrows parameter if there is a memory allocation problem.\n",
    "data = pd.read_table('./data/fra.txt',  usecols=range(2), names=['source', 'target'], nrows=8000)\n",
    "\n",
    "# Replace no-break and thin spaces in the target sentences.\n",
    "data.target = data.target.apply(lambda x: re.sub(u'\\xa0|\\u202f|\\u2009', u' ', x))\n",
    "data.sample(5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the _&lt;sos&gt;_ and _&lt;eos&gt;_ tokens are added to the target sentences at the beginning and the end, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two special tokens in the target sentences (start_of_stream/end_of_stream).\n",
    "data['target'] = '<sos> ' + data['target'] + ' <eos>'\n",
    "data.target[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s extract the vocabulary size of the sentences in the source and target corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocabulary of the source/target sentences.\n",
    "src_voc = sorted(list(data['source'].str.split(' ', expand=True).stack().unique()))\n",
    "trg_voc = sorted(list(data['target'].str.split(' ', expand=True).stack().unique()))\n",
    "\n",
    "# Get the vocabulary size for the source/target sentences.\n",
    "# Increase by one for the padding token.\n",
    "src_voc_size = len(src_voc) + 1\n",
    "trg_voc_size = len(trg_voc) + 1\n",
    "\n",
    "print(\"Vocabulary size of the source sentences:\", src_voc_size)\n",
    "print(\"Vocabulary size of the target sentences:\", trg_voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must obtain the maximum length of the sentences in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the maximum sentence length in the source/target sentences.\n",
    "max_src_len = max([(len(s.split(' '))) for s in data['source']])\n",
    "max_trg_len = max([(len(s.split(' '))) for s in data['target']])\n",
    "\n",
    "print(\"Maximum length of the source sentences:\", max_src_len)\n",
    "print(\"Maximum length of the target sentences:\", max_trg_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous statistics are essential as they let us configure the dimensions of the different units of the model.\n",
    "\n",
    "We need a dictionary to map numerical values to the output words for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word-to-index dictionary for the source/target tokens.\n",
    "# Zero index reserved for the padding token.\n",
    "src_word2idx = dict([(word, idx+1) for idx, word in enumerate(src_voc)])\n",
    "trg_word2idx = dict([(word, idx+1) for idx, word in enumerate(trg_voc)])\n",
    "\n",
    "print(trg_word2idx['Non'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the code for the index-to-word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index-to-word dictionary for the source/target tokens.\n",
    "src_idx2word = dict([(idx, word) for word, idx in src_word2idx.items()])\n",
    "trg_idx2word = dict([(idx, word) for word, idx in trg_word2idx.items()])\n",
    "\n",
    "print(trg_idx2word[675])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s examine the steps for creating the model by specifying the format of the input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The input/output data of the model.\n",
    "enc_input_data = np.zeros((len(data['source']), max_src_len), dtype='float32')\n",
    "dec_input_data = np.zeros((len(data['source']), max_trg_len), dtype='float32')\n",
    "dec_output_data = np.zeros((len(data['source']), max_trg_len, trg_voc_size), dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how _max_src_len_ and _max_trg_len_ determine the size of the created arrays. Next, we iterate over the whole dataset, transforming words into numerical values for the encoder’s and the decoder’s input while creating the one-hot array for the decoder’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the whole dataset.\n",
    "for i, (src_sentence, trg_sentence) in enumerate(zip(data['source'], data['target'])):\n",
    "    \n",
    "    # Create the input for the encoder.\n",
    "    for j, word in enumerate(src_sentence.split()):\n",
    "        enc_input_data[i, j] = src_word2idx[word]\n",
    " \n",
    "    # Create the input/output for the decoder.\n",
    "    for j, word in enumerate(trg_sentence.split()):\n",
    "        # Skip the '<eos>' word in the decoder input.\n",
    "        if j < len(trg_sentence.split())-1:\n",
    "            dec_input_data[i, j] = trg_word2idx[word]\n",
    "        # Skip the '<sos>' word in the decoder output.\n",
    "        if j > 0:\n",
    "            dec_output_data[i, j-1, trg_word2idx[word]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set up the architecture of the training model, which is a graph of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Create the input layers for the encoder/decoder.\n",
    "enc_input = Input(shape=(None,), dtype='float32',)\n",
    "dec_input = Input(shape=(None,), dtype='float32',)\n",
    "\n",
    "# Create the embedding layers for the encoder/decoder.\n",
    "embed_layer = Embedding(src_voc_size, 256, mask_zero=True)\n",
    "enc_embed = embed_layer(enc_input)\n",
    "embed_layer = Embedding(trg_voc_size, 256, mask_zero=True)\n",
    "dec_embed = embed_layer(dec_input)\n",
    "\n",
    "# Create the LSTM layers for the encoder/decoder.\n",
    "enc_LSTM = LSTM(256, return_state=True)\n",
    "_, state_h, state_c = enc_LSTM(enc_embed)\n",
    "dec_LSTM = LSTM(256, return_state=True, return_sequences=True)\n",
    "# The initial states of the decoder are the output from the encoder.\n",
    "dec_output, _, _ = dec_LSTM(dec_embed, initial_state=[state_h, state_c])\n",
    "\n",
    "# Create the output layer for the decoder.\n",
    "dec_dense = Dense(trg_voc_size, activation='softmax')\n",
    "dec_output = dec_dense(dec_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the pieces are in place, we can start constructing and training the complete encoder/decoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "The training model takes two inputs (_enc_input and dec_input_) and emits one output (_dec_output_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model.\n",
    "model = Model([enc_input, dec_input], dec_output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "model_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
    "print(\"Number of trainable parameters:\", model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of the complexity of the model, we print the number of its trainable parameters. In total, _4163282_ parameters must be estimated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s visualize the different layers of the training model that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Plot the model.\n",
    "plot_model(model, to_file='./images/model_plot.png', show_shapes=True, show_layer_names=True, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the whole architecture is in place, we start the training process by fitting the data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to the model.\n",
    "model.fit([enc_input_data, dec_input_data], dec_output_data,\n",
    "            batch_size=128, epochs=100, validation_split=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the validation_split parameter, 80% of the bilingual pairs are used for training, while the other 20% are used for testing. After _100_ epochs, we achieve an accuracy of around _88%_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the trained model for inference, we must change its architecture so that the output of each decoder timestep becomes an input to the subsequent one. The encoder remains unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to encode the input.\n",
    "enc_model = Model(enc_input, [state_h, state_c])\n",
    "\n",
    "# The hidden and cell states of the decoder at each step.\n",
    "dec_state_in_h = Input(shape=(256,))\n",
    "dec_state_in_c = Input(shape=(256,))\n",
    "\n",
    "# Set the embedding layer.\n",
    "dec_embed_2 = embed_layer(dec_input)\n",
    "\n",
    "# Set the LSTM layer.\n",
    "dec_output_2, dec_state_out_h, dec_state_out_c = dec_LSTM(dec_embed_2, initial_state=[dec_state_in_h, dec_state_in_c])\n",
    "\n",
    "# Set the output layer for the decoder.\n",
    "dec_output_2 = dec_dense(dec_output_2)\n",
    "\n",
    "# Create the decoder model.\n",
    "decoder_model = Model([dec_input] + [dec_state_in_h, dec_state_in_c],\n",
    "                        [dec_output_2] + [dec_state_out_h, dec_state_out_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must visualize the architecture of the altered decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model.\n",
    "plot_model(decoder_model, to_file='./images/inference_model_plot.png', show_shapes=True, show_layer_names=True, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it’s time to extract a translation for a given sentence. Here, we must include the necessary steps in the _getTranslation_ method and present them in detail. First, the method receives the index part of an example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate an input sentence.\n",
    "def getTranslation(index):\n",
    "\n",
    "    translation = word = ''\n",
    "\n",
    "    # Choose randomly a sequence from the data set.\n",
    "    #i = np.random.choice(len(data['source']))\n",
    "\n",
    "    # Choose a sequence from the data set.\n",
    "    source_seq = enc_input_data[index:index+1]\n",
    "\n",
    "    # Get the initial input states for the decoder.\n",
    "    states_h_c = enc_model.predict(source_seq)\n",
    "\n",
    "    # The first input token to the decoder is start_of_stream.\n",
    "    token = np.zeros((1,1))\n",
    "    token[0, 0] = trg_word2idx['<sos>']\n",
    "\n",
    "    # Start the decoding process.\n",
    "    while (word != '<eos>' and len(translation) <= 100):\n",
    "\n",
    "        # Predict the next token and states.\n",
    "        output, state_h, state_c = decoder_model.predict([token] + states_h_c)\n",
    "        \n",
    "        # Store the emitted token and the states for the next iteration.\n",
    "        idx = np.argmax(output[0, -1, :])\n",
    "        token[0, 0] = idx\n",
    "        states_h_c = [state_h, state_c]\n",
    "\n",
    "        # Extract the emitted word.\n",
    "        word = trg_idx2word[idx]\n",
    "        translation += ' ' + word\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s now time to call the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input sentence:\", data['source'][1308])\n",
    "print(\"Reference translation:\", data['target'][1308].replace(\"<sos>\",\"\").replace(\"<eos>\",\"\"))\n",
    "print(\"Hypothesis:\", getTranslation(1308)[:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An appropriate metric for evaluating the quality of MT systems is the `BiLingual Evaluation Understudy` (BLEU) score. Using BLEU, we compare the generated prediction to a reference sentence by counting matching n-grams in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "hypothesis = getTranslation(1006)[:-6].split()\n",
    "reference = data['target'][1006].replace(\"<sos>\",\"\").replace(\"<eos>\",\"\").split()\n",
    "\n",
    "# Calculate the BLEU score.\n",
    "bleu = sentence_bleu([reference], hypothesis, weights=(1, 1, 1))\n",
    "\n",
    "print(\"Input sentence:\", data['source'][1006])\n",
    "print(\"Reference translation:\", reference)\n",
    "print(\"Hypothesis:\", hypothesis)\n",
    "print(\"BLEU score:\", bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have learned …\n",
    "\n",
    "| | |\n",
    "| --- | --- |\n",
    "| **ML algorithms & models**<ul><li>Recurrent Neural Networks</li></ul> |  **Performance**<ul><li>BLEU score</li></ul> |\n",
    "| | |"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "5ec7f24bccdc5982815fc9e679db841794fd8b692b70ffe03fd828d6ea20f1fe"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
