{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 7</u>: Summarizing Wikipedia Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "required_packages = {'scrapy', 'wikipedia', 'wikipedia_sections'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``XML path`` (XPath) is an alternative to the CSS selectors used so far in ``scrapy``, a language for selecting tags in XML documents and HTML. This section aims to provide the first flavor on this language, which per se requires an individual book. As before we implement the spider, set the start URL and define the ``parse`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "# Create a spider for scraping Wikipedia articles.\n",
    "class WikipediaSpider(scrapy.Spider):\n",
    "    name = 'wikipedia_spider'\n",
    "    allowed_domains = ['en.wikipedia.org']\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Athens']\n",
    "    \n",
    "    # Parse the info for a specific page.\n",
    "    def parse(self, response):\n",
    "\n",
    "        print(response.xpath(\"//span[@class='mw-headline']/text()\").getall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, starting the crawler yields all headlines from the specific Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create a crawler process using the quote spider.\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# Start the crawling.\n",
    "crawler = process.create_crawler(WikipediaSpider)\n",
    "process.crawl(crawler)\n",
    "process.start()\n",
    "\n",
    "# In case you get: ReactorNotRestartable error, you have to restart the kernel.\n",
    "# The reactor is only meant to run once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same sections as with the scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Techniques for Text \n",
    "&copy;2022&ndash;2023, Nikos Tsourakis, <nikos@tsourakis.net>, Packt Publications. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "5ec7f24bccdc5982815fc9e679db841794fd8b692b70ffe03fd828d6ea20f1fe"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
