{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 7</u>: Summarizing Wikipedia Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we scrape text data from [Wikipedia](https://en.wikipedia.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "required_packages = {'scrapy', 'wikipedia'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Note</ins>: In case you get a _ReactorNotRestartable_ error, you have to restart the kernel. The reactor is only meant to run once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``XML path`` (XPath) is an alternative to the CSS selectors used so far in ``scrapy``, a language for selecting tags in XML documents and HTML. We implement the spider, set the start URL and define the ``parse`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "# Create a spider for scraping Wikipedia articles.\n",
    "class WikipediaSpider(scrapy.Spider):\n",
    "    name = 'wikipedia_spider'\n",
    "    allowed_domains = ['en.wikipedia.org']\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Athens']\n",
    "    \n",
    "    # Parse the info for a specific page.\n",
    "    def parse(self, response):\n",
    "\n",
    "        print(response.xpath(\"//span[@class='mw-headline']/text()\").getall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, starting the crawler yields all headlines from the specific Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 12:44:29 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-11-07 12:44:29 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform Windows-10-10.0.19042-SP0\n",
      "2022-11-07 12:44:29 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2022-11-07 12:44:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-11-07 12:44:29 [scrapy.extensions.telnet] INFO: Telnet Password: 25313d2eb5206d45\n",
      "2022-11-07 12:44:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-11-07 12:44:29 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-11-07 12:44:29 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-11-07 12:44:29 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-11-07 12:44:29 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-11-07 12:44:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-11-07 12:44:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-11-07 12:44:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Athens> (referer: None)\n",
      "2022-11-07 12:44:30 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-11-07 12:44:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 243,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 161245,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.648767,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 11, 7, 11, 44, 30, 543513),\n",
      " 'httpcompression/response_bytes': 878933,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 11, 7, 11, 44, 29, 894746)}\n",
      "2022-11-07 12:44:30 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Etymology and names', 'History', 'Geography', 'Environment', 'Safety', 'Climate', 'Locations', 'Neighbourhoods of the center of Athens (Municipality of Athens)', 'Parks and zoos', 'Urban and suburban municipalities', 'Administration', 'Athens Urban Area', 'Athens Metropolitan Area', 'Demographics', 'Population in modern times', 'Population of the Athens Metropolitan Area', 'Population in ancient times', 'Government and politics', 'International relations and influence', 'Twin towns – sister cities', 'Partnerships', 'Other locations named after Athens', 'Economy', 'Transport', 'Bus transport', 'Athens Metro', 'Commuter/suburban rail (Proastiakos)', 'Tram', 'Athens International Airport', 'Railways and ferry connections', 'Motorways', 'Education', 'Culture', 'Archaeological hub', 'Architecture', 'Urban sculpture', 'Museums', 'Tourism', 'Entertainment and performing arts', 'Sports', 'Sports clubs', 'Olympic Games', '1896 Summer Olympics', '1906 Summer Olympics', '2004 Summer Olympics', 'See also', 'References', 'External links']\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create a crawler process using the quote spider.\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# Start the crawling.\n",
    "crawler = process.create_crawler(WikipediaSpider)\n",
    "process.crawl(crawler)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have learned …\n",
    "\n",
    "| |\n",
    "| --- |\n",
    "| **Tools**<ul><li>Web crawling and scraping</li></ul>\n",
    "| |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "5ec7f24bccdc5982815fc9e679db841794fd8b692b70ffe03fd828d6ea20f1fe"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
