{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Chapter 9</u>: Generating Text in Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Find out which packages are missing.\n",
    "installed_packages = {dist.key for dist in pkg_resources.working_set}\n",
    "required_packages = {'torch', 'transformers', 'trl'}\n",
    "missing_packages = required_packages - installed_packages\n",
    "\n",
    "# If there are missing packages install them.\n",
    "if missing_packages:\n",
    "    print('Installing the following packages: ' + str(missing_packages))\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing_packages], stdout=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the pre-trained model using reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the `Transformer Reinforcement Learning` (trl) library that allows the training of transformer language models with `Proximal Policy Optimization` (PPO). \n",
    "\n",
    "The small version of the _DialoGPT_ model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at microsoft/DialoGPT-small and are newly initialized: ['transformer.h.4.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'v_head.summary.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'v_head.summary.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.3.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at microsoft/DialoGPT-small and are newly initialized: ['transformer.h.4.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'v_head.summary.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'v_head.summary.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.3.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "from trl.ppo import PPOTrainer\n",
    "\n",
    "# Load the models.\n",
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained('microsoft/DialoGPT-small')\n",
    "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained('microsoft/DialoGPT-small')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the _chat_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with the bot using a new input and the previous history.\n",
    "def chat(input, history=[], gen_kwargs=[]):\n",
    "    \n",
    "    # Tokenize the input.\n",
    "    new_user_input_ids = gpt2_tokenizer.encode(input+gpt2_tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # Update the dialogue history.\n",
    "    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n",
    "\n",
    "    # Generate the response of the bot.\n",
    "    new_history = gpt2_model.generate(bot_input_ids, **gen_kwargs).tolist()\n",
    "\n",
    "    # Convert the tokens to text.\n",
    "    output = gpt2_tokenizer.decode(new_history[0]).split(\"<|endoftext|>\")\n",
    "    output = [(output[i], output[i+1]) for i in range(0, len(output)-1, 2)]\n",
    "    return output, new_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the parameters for the model and initializat the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model.\n",
    "gen_kwargs = {\n",
    "    \"max_length\":1000,\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": gpt2_tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "# Initialize the trainer.\n",
    "ppo_config = {'batch_size': 1, 'forward_batch_size': 1}\n",
    "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, gpt2_tokenizer, **ppo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query is one of the elements for the reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a query.\n",
    "query_txt = \"Does money buy happiness?\"\n",
    "query_tensor = gpt2_tokenizer.encode(query_txt+gpt2_tokenizer.eos_token, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform ten tnteraction that will help tuning the language model. In practice many more interactions are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- reward: If you keep barney top 10, idk if it's even cheaper\n",
      "+ reward: Yes, but before there was always spend money how can you get happiness. Get happiness, get your money for it.\n",
      "- reward: Megan convinces Barbel that the advice she was giving was good. In retrospect, they were the right choices.\n",
      "+ reward: Money buys happiness.\n",
      "+ reward: Money buy happiness?\n",
      "+ reward: . can i buy happiness from your family?\n",
      "+ reward: Money buys happiness. Money buys happiness. Money doesn't buy happiness. What trends should we look to bring with our little robot uprising? Money Is Happiness.\n",
      "+ reward: money buy happiness religion wage happiness\n",
      "+ reward: Money buy happiness. Any number guys... anyone? Please?\n",
      "- reward: Why would you come live with someone else when you can have this mother?\n"
     ]
    }
   ],
   "source": [
    "# Repeat the training for 10 interactions.\n",
    "for x in range(10):\n",
    "\n",
    "    response_tensors = []\n",
    "    pipe_outputs = []\n",
    "\n",
    "    # Get a reposnse from the chatbot.\n",
    "    result, history = chat(query_txt, [], gen_kwargs)\n",
    "    response_txt = result[0][1]\n",
    "    response_tensor = gpt2_tokenizer.encode(response_txt+gpt2_tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    \n",
    "    # Positive reward.\n",
    "    if response_txt.find('happy') >= 0 or response_txt.find('happiness') >= 0 or response_txt.find('fun') >= 0:\n",
    "        print(\"+ reward: \" + response_txt)\n",
    "        reward = [torch.tensor(1.0)]\n",
    "    # Negative reward.\n",
    "    else:\n",
    "        print(\"- reward: \" + response_txt)\n",
    "        reward = [torch.tensor(-1.0)]\n",
    "\n",
    "    # Train the model with the ppo algorithm.\n",
    "    train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have learned â€¦\n",
    "\n",
    "| |\n",
    "| --- |\n",
    "| **ML concepts** <ul><li>Fine-tuning</li><li>Reinforcement learning</li></ul> |\n",
    "| |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "34c3ec88db1a123a786d67d086f3ede88281b71e687e4350202a680e0c5fcbcd"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "8f1e200aa4e9598f1b1017d8bb6526388dc3fae44f5def43455ba665e800f8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
